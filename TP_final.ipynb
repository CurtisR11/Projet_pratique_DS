{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "533f33ab",
      "metadata": {
        "id": "533f33ab"
      },
      "source": [
        "# TP1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3680627c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3680627c",
        "outputId": "16747611-af65-4b3b-fb5b-30aafb122a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▶ Scraping des ratios financiers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41/41 [00:49<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Ratios financiers sauvegardés dans /root/Desktop/Projet_Final_DS/data/ratios.csv\n",
            "▶ Début du scraping des variations boursières...\n",
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Apple -> /root/Desktop/Projet_Final_DS/data/companies/Apple.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Microsoft -> /root/Desktop/Projet_Final_DS/data/companies/Microsoft.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Amazon -> /root/Desktop/Projet_Final_DS/data/companies/Amazon.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Alphabet -> /root/Desktop/Projet_Final_DS/data/companies/Alphabet.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Meta -> /root/Desktop/Projet_Final_DS/data/companies/Meta.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Tesla -> /root/Desktop/Projet_Final_DS/data/companies/Tesla.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour NVIDIA -> /root/Desktop/Projet_Final_DS/data/companies/NVIDIA.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Samsung -> /root/Desktop/Projet_Final_DS/data/companies/Samsung.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Tencent -> /root/Desktop/Projet_Final_DS/data/companies/Tencent.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Alibaba -> /root/Desktop/Projet_Final_DS/data/companies/Alibaba.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour IBM -> /root/Desktop/Projet_Final_DS/data/companies/IBM.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Intel -> /root/Desktop/Projet_Final_DS/data/companies/Intel.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Oracle -> /root/Desktop/Projet_Final_DS/data/companies/Oracle.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Sony -> /root/Desktop/Projet_Final_DS/data/companies/Sony.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Adobe -> /root/Desktop/Projet_Final_DS/data/companies/Adobe.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Netflix -> /root/Desktop/Projet_Final_DS/data/companies/Netflix.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour AMD -> /root/Desktop/Projet_Final_DS/data/companies/AMD.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Qualcomm -> /root/Desktop/Projet_Final_DS/data/companies/Qualcomm.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Cisco -> /root/Desktop/Projet_Final_DS/data/companies/Cisco.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour JP Morgan -> /root/Desktop/Projet_Final_DS/data/companies/JP_Morgan.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Goldman Sachs -> /root/Desktop/Projet_Final_DS/data/companies/Goldman_Sachs.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Visa -> /root/Desktop/Projet_Final_DS/data/companies/Visa.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Johnson & Johnson -> /root/Desktop/Projet_Final_DS/data/companies/Johnson_&_Johnson.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Pfizer -> /root/Desktop/Projet_Final_DS/data/companies/Pfizer.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour ExxonMobil -> /root/Desktop/Projet_Final_DS/data/companies/ExxonMobil.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour ASML -> /root/Desktop/Projet_Final_DS/data/companies/ASML.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour SAP -> /root/Desktop/Projet_Final_DS/data/companies/SAP.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Siemens -> /root/Desktop/Projet_Final_DS/data/companies/Siemens.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Louis Vuitton (LVMH) -> /root/Desktop/Projet_Final_DS/data/companies/Louis_Vuitton_(LVMH).csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour TotalEnergies -> /root/Desktop/Projet_Final_DS/data/companies/TotalEnergies.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Shell -> /root/Desktop/Projet_Final_DS/data/companies/Shell.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Baidu -> /root/Desktop/Projet_Final_DS/data/companies/Baidu.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour JD.com -> /root/Desktop/Projet_Final_DS/data/companies/JD.com.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour BYD -> /root/Desktop/Projet_Final_DS/data/companies/BYD.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour ICBC -> /root/Desktop/Projet_Final_DS/data/companies/ICBC.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Toyota -> /root/Desktop/Projet_Final_DS/data/companies/Toyota.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour SoftBank -> /root/Desktop/Projet_Final_DS/data/companies/SoftBank.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Nintendo -> /root/Desktop/Projet_Final_DS/data/companies/Nintendo.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Hyundai -> /root/Desktop/Projet_Final_DS/data/companies/Hyundai.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Reliance Industries -> /root/Desktop/Projet_Final_DS/data/companies/Reliance_Industries.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Données exportées pour Tata Consultancy Services -> /root/Desktop/Projet_Final_DS/data/companies/Tata_Consultancy_Services.csv\n",
            "\n",
            "📢 *Récapitulatif final*\n",
            "✅ Nombre de fichiers exportés avec succès : 41/41\n",
            "\n",
            "🎉 Toutes les entreprises ont été récupérées avec succès !\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import time\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "companies = {\n",
        "    \"Apple\": \"AAPL\",\n",
        "    \"Microsoft\": \"MSFT\",\n",
        "    \"Amazon\": \"AMZN\",\n",
        "    \"Alphabet\": \"GOOGL\",\n",
        "    \"Meta\": \"META\",\n",
        "    \"Tesla\": \"TSLA\",\n",
        "    \"NVIDIA\": \"NVDA\",\n",
        "    \"Samsung\": \"005930.KS\",\n",
        "    \"Tencent\": \"TCEHY\",\n",
        "    \"Alibaba\": \"BABA\",\n",
        "    \"IBM\": \"IBM\",\n",
        "    \"Intel\": \"INTC\",\n",
        "    \"Oracle\": \"ORCL\",\n",
        "    \"Sony\": \"SONY\",\n",
        "    \"Adobe\": \"ADBE\",\n",
        "    \"Netflix\": \"NFLX\",\n",
        "    \"AMD\": \"AMD\",\n",
        "    \"Qualcomm\": \"QCOM\",\n",
        "    \"Cisco\": \"CSCO\",\n",
        "    \"JP Morgan\": \"JPM\",\n",
        "    \"Goldman Sachs\": \"GS\",\n",
        "    \"Visa\": \"V\",\n",
        "    \"Johnson & Johnson\": \"JNJ\",\n",
        "    \"Pfizer\": \"PFE\",\n",
        "    \"ExxonMobil\": \"XOM\",\n",
        "    \"ASML\": \"ASML.AS\",\n",
        "    \"SAP\": \"SAP.DE\",\n",
        "    \"Siemens\": \"SIE.DE\",\n",
        "    \"Louis Vuitton (LVMH)\": \"MC.PA\",\n",
        "    \"TotalEnergies\": \"TTE.PA\",\n",
        "    \"Shell\": \"SHEL.L\",\n",
        "    \"Baidu\": \"BIDU\",\n",
        "    \"JD.com\": \"JD\",\n",
        "    \"BYD\": \"BYDDY\",\n",
        "    \"ICBC\": \"1398.HK\",\n",
        "    \"Toyota\": \"TM\",\n",
        "    \"SoftBank\": \"9984.T\",\n",
        "    \"Nintendo\": \"NTDOY\",\n",
        "    \"Hyundai\": \"HYMTF\",\n",
        "    \"Reliance Industries\": \"RELIANCE.NS\",\n",
        "    \"Tata Consultancy Services\": \"TCS.NS\"\n",
        "}\n",
        "\n",
        "def safe_get_ticker_info(symbol, retries=1, delay=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            return yf.Ticker(symbol).info\n",
        "        except Exception as e:\n",
        "            if \"Rate limited\" in str(e):\n",
        "                print(f\"⏳ Rate limited pour {symbol}. Nouvelle tentative dans {delay} secondes...\")\n",
        "                time.sleep(delay)\n",
        "                delay *= 2\n",
        "            else:\n",
        "                raise e\n",
        "    raise Exception(f\"❌ Échec après {retries} tentatives pour {symbol}\")\n",
        "\n",
        "def scrape_financial_ratios(output_path):\n",
        "    print(\"▶ Scraping des ratios financiers...\")\n",
        "    ratios = {\n",
        "        \"forwardPE\": [], \"beta\": [], \"priceToBook\": [], \"priceToSales\": [],\n",
        "        \"dividendYield\": [], \"trailingEps\": [], \"debtToEquity\": [],\n",
        "        \"currentRatio\": [], \"quickRatio\": [], \"returnOnEquity\": [],\n",
        "        \"returnOnAssets\": [], \"operatingMargins\": [], \"profitMargins\": []\n",
        "    }\n",
        "\n",
        "    company_names = []\n",
        "\n",
        "    for company_name, symbol in tqdm(companies.items()):\n",
        "        try:\n",
        "            info = safe_get_ticker_info(symbol)\n",
        "            company_names.append(company_name)\n",
        "            for ratio_key in ratios.keys():\n",
        "                value = info.get(ratio_key, None)\n",
        "                ratios[ratio_key].append(value)\n",
        "            time.sleep(1)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur pour {company_name}: {e}\")\n",
        "            for ratio_key in ratios:\n",
        "                ratios[ratio_key].append(None)\n",
        "            company_names.append(company_name)\n",
        "\n",
        "    df_ratios = pd.DataFrame(ratios, index=company_names)\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    df_ratios.to_csv(output_path, encoding='utf-8')\n",
        "    print(f\"✅ Ratios financiers sauvegardés dans {output_path}\")\n",
        "\n",
        "def fetch_stock_variations(company_name, symbol, start_date, end_date, output_folder):\n",
        "    try:\n",
        "        df = yf.download(symbol, start=start_date, end=end_date)\n",
        "        df.columns = df.columns.droplevel(1)\n",
        "        df.columns.name = None\n",
        "        df.index.name = None\n",
        "        df = df[[\"Close\"]]\n",
        "        df = df.reset_index()\n",
        "        df.rename(columns={\"index\": \"Date\"}, inplace=True)\n",
        "\n",
        "        if df.empty:\n",
        "            raise ValueError(\"DataFrame vide – possible rate limit ou symbole invalide.\")\n",
        "\n",
        "        #df = df[['Close']].copy()\n",
        "\n",
        "        #  Transforme l'index (Date) en colonne explicite\n",
        "\n",
        "        # Conversion sécurisée de 'Close' en float\n",
        "        df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
        "        df.dropna(subset=['Close'], inplace=True)\n",
        "\n",
        "        df['Next_Day_Close'] = df['Close'].shift(-1)\n",
        "        df['Daily_Return'] = df['Close'].pct_change()\n",
        "        df.dropna(inplace=True)\n",
        "\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "        file_path = os.path.join(output_folder, f\"{company_name.replace(' ', '_')}.csv\")\n",
        "        df.to_csv(file_path)\n",
        "        time.sleep(1)\n",
        "        return True, company_name, file_path\n",
        "    except Exception as e:\n",
        "        return False, company_name, str(e)\n",
        "\n",
        "def main():\n",
        "    desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "    project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "    os.makedirs(project_path, exist_ok=True)\n",
        "\n",
        "    start_date = (datetime.today() - timedelta(days=365*5)).strftime(\"%Y-%m-%d\")\n",
        "    end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "    output_folder = os.path.join(project_path, \"data\", \"companies\")\n",
        "    ratios_path = os.path.join(project_path, \"data\", \"ratios.csv\")\n",
        "\n",
        "    # Partie 1 : Ratios financiers\n",
        "    scrape_financial_ratios(ratios_path)\n",
        "\n",
        "    # Partie 2 : Variations journalières\n",
        "    success_count = 0\n",
        "    failed_companies = []\n",
        "\n",
        "    print(\"▶ Début du scraping des variations boursières...\")\n",
        "    for company_name, symbol in companies.items():\n",
        "        success, name, result = fetch_stock_variations(company_name, symbol, start_date, end_date, output_folder)\n",
        "        if success:\n",
        "            success_count += 1\n",
        "            print(f\"✅ Données exportées pour {name} -> {result}\")\n",
        "        else:\n",
        "            print(f\"❌ Erreur pour {name} : {result}\")\n",
        "            failed_companies.append(name)\n",
        "\n",
        "    print(\"\\n📢 *Récapitulatif final*\")\n",
        "    print(f\"✅ Nombre de fichiers exportés avec succès : {success_count}/{len(companies)}\")\n",
        "\n",
        "    if failed_companies:\n",
        "        print(\"\\n❗ Entreprises non récupérées :\")\n",
        "        for failed in failed_companies:\n",
        "            print(f\"  - {failed}\")\n",
        "    else:\n",
        "        print(\"\\n🎉 Toutes les entreprises ont été récupérées avec succès !\")\n",
        "#\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aae431b",
      "metadata": {
        "id": "6aae431b"
      },
      "source": [
        "# TP2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"json_files/AMD_news.json\" ) as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "ZhINxQevz4Lh"
      },
      "id": "ZhINxQevz4Lh",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3XnsPN_QzgDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f8d512-6805-4f67-c28e-a76361f791ce"
      },
      "id": "3XnsPN_QzgDA",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls /root/Desktop/Projet_Final_DS/data/companies/"
      ],
      "metadata": {
        "id": "ILY1l0Rz0S4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ce6c2e-cd92-48f5-bafe-863c56a359fb"
      },
      "id": "ILY1l0Rz0S4V",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Adobe.csv\t     ICBC.csv\t\t\t Reliance_Industries.csv\n",
            " Alibaba.csv\t     Intel.csv\t\t\t Samsung.csv\n",
            " Alphabet.csv\t     JD.com.csv\t\t\t SAP.csv\n",
            " Amazon.csv\t    'Johnson_&_Johnson.csv'\t Shell.csv\n",
            " AMD.csv\t     JP_Morgan.csv\t\t Siemens.csv\n",
            " Apple.csv\t    'Louis_Vuitton_(LVMH).csv'\t SoftBank.csv\n",
            " ASML.csv\t     Meta.csv\t\t\t Sony.csv\n",
            " Baidu.csv\t     Microsoft.csv\t\t Tata_Consultancy_Services.csv\n",
            " BYD.csv\t     Netflix.csv\t\t Tencent.csv\n",
            " Cisco.csv\t     Nintendo.csv\t\t Tesla.csv\n",
            " ExxonMobil.csv      NVIDIA.csv\t\t\t TotalEnergies.csv\n",
            " Goldman_Sachs.csv   Oracle.csv\t\t\t Toyota.csv\n",
            " Hyundai.csv\t     Pfizer.csv\t\t\t Visa.csv\n",
            " IBM.csv\t     Qualcomm.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1704e958",
      "metadata": {
        "id": "1704e958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75893451-a1fc-4aca-fa64-3568ec524564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-1a4e88e0a15d>:25: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_returns = pd.DataFrame(all_returns).fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Résultats pour : Financial Profile\n",
            "KMeans Silhouette Score      : 0.359\n",
            "Hierarchical Silhouette Score: 0.367\n",
            "DBSCAN Silhouette Score      : 0.436\n",
            "✅ Meilleur algo : DBSCAN\n",
            "✅ Données + labels sauvegardés : /root/Desktop/Projet_Final_DS/outputs/clustering/financial_profile_labels.csv\n",
            "📸 TSNE sauvegardé : financial_profile_kmeans_tsne.png\n",
            "📸 TSNE sauvegardé : financial_profile_hierarchical_tsne.png\n",
            "📸 TSNE sauvegardé : financial_profile_dbscan_tsne.png\n",
            "\n",
            "📊 Résultats pour : Risk Profile\n",
            "KMeans Silhouette Score      : 0.526\n",
            "Hierarchical Silhouette Score: 0.571\n",
            "DBSCAN Silhouette Score      : 0.767\n",
            "✅ Meilleur algo : DBSCAN\n",
            "✅ Données + labels sauvegardés : /root/Desktop/Projet_Final_DS/outputs/clustering/risk_profile_labels.csv\n",
            "📸 TSNE sauvegardé : risk_profile_kmeans_tsne.png\n",
            "📸 TSNE sauvegardé : risk_profile_hierarchical_tsne.png\n",
            "📸 TSNE sauvegardé : risk_profile_dbscan_tsne.png\n",
            "\n",
            "📊 Résultats pour : Daily Returns Correlation\n",
            "KMeans Silhouette Score      : 0.261\n",
            "Hierarchical Silhouette Score: 0.286\n",
            "DBSCAN Silhouette Score      : 0.257\n",
            "✅ Meilleur algo : Hierarchical\n",
            "✅ Données + labels sauvegardés : /root/Desktop/Projet_Final_DS/outputs/clustering/daily_returns_correlation_labels.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/cluster/hierarchy.py:796: ClusterWarning: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
            "  return linkage(y, method='ward', metric='euclidean')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📸 TSNE sauvegardé : daily_returns_correlation_kmeans_tsne.png\n",
            "📸 TSNE sauvegardé : daily_returns_correlation_hierarchical_tsne.png\n",
            "📸 TSNE sauvegardé : daily_returns_correlation_dbscan_tsne.png\n",
            "📄 Résumé des scores sauvegardé dans : /root/Desktop/Projet_Final_DS/outputs/clustering/clustering_results.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def preprocess_for_financial_clustering(filepath, selected_columns):\n",
        "    df = pd.read_csv(filepath, index_col=0)\n",
        "    df_clean = df[selected_columns].dropna()\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = scaler.fit_transform(df_clean)\n",
        "    return pd.DataFrame(data_scaled, index=df_clean.index, columns=df_clean.columns), df_clean\n",
        "\n",
        "def prepare_daily_returns(folder_path):\n",
        "    all_returns = {}\n",
        "    for filepath in glob.glob(os.path.join(folder_path, \"*.csv\")):\n",
        "        company = os.path.basename(filepath).replace(\".csv\", \"\").replace(\"_\", \" \")\n",
        "        df = pd.read_csv(filepath)\n",
        "        if \"Daily_Return\" in df.columns:\n",
        "            all_returns[company] = df[\"Daily_Return\"]\n",
        "    df_returns = pd.DataFrame(all_returns).fillna(method='ffill').fillna(method='bfill')\n",
        "    return df_returns.corr()\n",
        "\n",
        "def do_kmeans(data, n_clusters):\n",
        "    model = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    labels = model.fit_predict(data)\n",
        "    return labels, silhouette_score(data, labels)\n",
        "\n",
        "def do_hierarchical(data, n_clusters):\n",
        "    model = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    labels = model.fit_predict(data)\n",
        "    return labels, silhouette_score(data, labels)\n",
        "\n",
        "def do_dbscan(data, eps=1, min_samples=2):\n",
        "    model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels = model.fit_predict(data)\n",
        "    try:\n",
        "        score = silhouette_score(data, labels) if len(set(labels)) > 1 else -1\n",
        "    except:\n",
        "        score = -1\n",
        "    return labels, score\n",
        "\n",
        "def save_labels_with_data(df_data, labels_dict, dataset_name, output_path):\n",
        "    df_out = df_data.copy()\n",
        "    for algo, labels in labels_dict.items():\n",
        "        df_out[algo + \"_Cluster\"] = labels\n",
        "    filename = dataset_name.lower().replace(\" \", \"_\") + \"_labels.csv\"\n",
        "    full_path = os.path.join(output_path, filename)\n",
        "    df_out.to_csv(full_path)\n",
        "    print(f\"✅ Données + labels sauvegardés : {full_path}\")\n",
        "\n",
        "def plot_tsne(data_scaled, labels_dict, dataset_name, output_path):\n",
        "    tsne = TSNE(n_components=2, random_state=42, init=\"random\", perplexity=5)\n",
        "    tsne_results = tsne.fit_transform(data_scaled)\n",
        "\n",
        "    for algo, clusters in labels_dict.items():\n",
        "        df_tsne = pd.DataFrame(tsne_results, columns=[\"TSNE1\", \"TSNE2\"])\n",
        "        df_tsne[\"Cluster\"] = clusters\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        unique_clusters = np.unique(clusters)\n",
        "        colors = plt.colormaps.get_cmap(\"tab10\")\n",
        "\n",
        "        for i, cluster in enumerate(unique_clusters):\n",
        "            subset = df_tsne[df_tsne[\"Cluster\"] == cluster]\n",
        "            plt.scatter(subset[\"TSNE1\"], subset[\"TSNE2\"],\n",
        "                        label=f\"Cluster {cluster}\",\n",
        "                        color=colors(i % 10),\n",
        "                        alpha=0.7)\n",
        "\n",
        "        plt.xlabel(\"TSNE 1\")\n",
        "        plt.ylabel(\"TSNE 2\")\n",
        "        plt.title(f\"TSNE - {dataset_name} - {algo}\")\n",
        "        plt.legend()\n",
        "        filename = f\"{dataset_name.lower().replace(' ', '_')}_{algo.lower()}_tsne.png\"\n",
        "        plt.savefig(os.path.join(output_path, filename))\n",
        "        plt.close()\n",
        "        print(f\"📸 TSNE sauvegardé : {filename}\")\n",
        "\n",
        "def main():\n",
        "    desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "    project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "    ratios_path = os.path.join(project_path, \"data\", \"ratios.csv\")\n",
        "    returns_folder = os.path.join(project_path, \"data\", \"companies\")\n",
        "    output_path = os.path.join(project_path, \"outputs\", \"clustering\")\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    selected_columns_finance = [\"forwardPE\", \"beta\", \"priceToBook\", \"returnOnEquity\"]\n",
        "    selected_columns_risk = [\"debtToEquity\", \"currentRatio\", \"quickRatio\"]\n",
        "\n",
        "    data_finance_scaled, data_finance_raw = preprocess_for_financial_clustering(ratios_path, selected_columns_finance)\n",
        "    data_risk_scaled, data_risk_raw = preprocess_for_financial_clustering(ratios_path, selected_columns_risk)\n",
        "    data_returns = prepare_daily_returns(returns_folder)\n",
        "    data_returns_dist = 1 - data_returns\n",
        "\n",
        "    datasets = {\n",
        "        \"Financial Profile\": (data_finance_scaled, data_finance_raw),\n",
        "        \"Risk Profile\": (data_risk_scaled, data_risk_raw),\n",
        "        \"Daily Returns Correlation\": (data_returns_dist, data_returns)\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for name, (data_scaled, data_original) in datasets.items():\n",
        "        print(f\"\\n📊 Résultats pour : {name}\")\n",
        "        try:\n",
        "            labels_km, score_km = do_kmeans(data_scaled, n_clusters=5)\n",
        "            labels_hc, score_hc = do_hierarchical(data_scaled, n_clusters=5)\n",
        "            labels_db, score_db = do_dbscan(data_scaled, eps=1, min_samples=2)\n",
        "\n",
        "            all_scores = {\n",
        "                \"KMeans\": score_km,\n",
        "                \"Hierarchical\": score_hc,\n",
        "                \"DBSCAN\": score_db\n",
        "            }\n",
        "            best_algo = max(all_scores, key=all_scores.get)\n",
        "\n",
        "            print(f\"KMeans Silhouette Score      : {score_km:.3f}\")\n",
        "            print(f\"Hierarchical Silhouette Score: {score_hc:.3f}\")\n",
        "            print(f\"DBSCAN Silhouette Score      : {score_db:.3f}\")\n",
        "            print(f\"✅ Meilleur algo : {best_algo}\")\n",
        "\n",
        "            results.append({\n",
        "                \"Dataset\": name,\n",
        "                \"KMeans Silhouette\": score_km,\n",
        "                \"Hierarchical Silhouette\": score_hc,\n",
        "                \"DBSCAN Silhouette\": score_db,\n",
        "                \"Best Algorithm\": best_algo\n",
        "            })\n",
        "\n",
        "            labels_dict = {\n",
        "                \"KMeans\": labels_km,\n",
        "                \"Hierarchical\": labels_hc,\n",
        "                \"DBSCAN\": labels_db\n",
        "            }\n",
        "\n",
        "            save_labels_with_data(data_original, labels_dict, name, output_path)\n",
        "            plot_tsne(data_scaled, labels_dict, name, output_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur sur {name} : {e}\")\n",
        "            results.append({\n",
        "                \"Dataset\": name,\n",
        "                \"KMeans Silhouette\": None,\n",
        "                \"Hierarchical Silhouette\": None,\n",
        "                \"DBSCAN Silhouette\": None,\n",
        "                \"Best Algorithm\": f\"Erreur: {e}\"\n",
        "            })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_csv_path = os.path.join(output_path, \"clustering_results.csv\")\n",
        "    results_df.to_csv(results_csv_path, index=False)\n",
        "    print(f\"📄 Résumé des scores sauvegardé dans : {results_csv_path}\")\n",
        "\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf38040e",
      "metadata": {
        "id": "cf38040e"
      },
      "source": [
        "# TP 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ta"
      ],
      "metadata": {
        "id": "lh9ESMRL3U0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0850ab78-31c4-409b-f816-2c35dfbc694a"
      },
      "id": "lh9ESMRL3U0k",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=4a83198c94183d5b2f1391edb619d7d37f425a65554cef39d0745fd106a0ac4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n"
      ],
      "metadata": {
        "id": "lIox0IqO7PTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a91a5760-4dca-4dc8-c0a7-41645fdfadd2"
      },
      "id": "lIox0IqO7PTR",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4cc3793",
      "metadata": {
        "id": "b4cc3793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "809e0c37-738e-4a69-b77a-d672d57afd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training XGBoost...\n",
            "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
            "[CV 1/2] END learning_rate=0.01, n_estimators=10;, score=0.537 total time=   1.0s\n",
            "[CV 2/2] END learning_rate=0.01, n_estimators=10;, score=0.544 total time=   1.8s\n",
            "[CV 1/2] END learning_rate=0.01, n_estimators=40;, score=0.555 total time=   5.2s\n",
            "[CV 2/2] END learning_rate=0.01, n_estimators=40;, score=0.557 total time=   4.2s\n",
            "[CV 1/2] END learning_rate=0.1, n_estimators=10;, score=0.562 total time=   1.6s\n",
            "[CV 2/2] END learning_rate=0.1, n_estimators=10;, score=0.577 total time=   0.7s\n",
            "[CV 1/2] END learning_rate=0.1, n_estimators=40;, score=0.605 total time=   3.8s\n",
            "[CV 2/2] END learning_rate=0.1, n_estimators=40;, score=0.615 total time=   1.1s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.36      0.48       994\n",
            "           1       0.59      0.89      0.71      2359\n",
            "           2       0.71      0.38      0.50      1443\n",
            "\n",
            "    accuracy                           0.62      4796\n",
            "   macro avg       0.68      0.54      0.56      4796\n",
            "weighted avg       0.66      0.62      0.60      4796\n",
            "\n",
            "Accuracy: 0.62\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 550x340 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training KNN...\n",
            "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
            "[CV 1/2] END .....................n_neighbors=3;, score=0.488 total time=   1.5s\n",
            "[CV 2/2] END .....................n_neighbors=3;, score=0.478 total time=   1.6s\n",
            "[CV 1/2] END .....................n_neighbors=5;, score=0.493 total time=   1.8s\n",
            "[CV 2/2] END .....................n_neighbors=5;, score=0.495 total time=   0.9s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.42      0.40       994\n",
            "           1       0.57      0.69      0.62      2359\n",
            "           2       0.51      0.31      0.39      1443\n",
            "\n",
            "    accuracy                           0.52      4796\n",
            "   macro avg       0.49      0.47      0.47      4796\n",
            "weighted avg       0.51      0.52      0.51      4796\n",
            "\n",
            "Accuracy: 0.52\n",
            "SHAP summary plot not available for KNeighborsClassifier model.\n",
            "Training SVM...\n",
            "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
            "[CV 1/2] END ..............C=0.1, kernel=linear;, score=0.489 total time=   8.7s\n",
            "[CV 2/2] END ..............C=0.1, kernel=linear;, score=0.489 total time=   9.2s\n",
            "[CV 1/2] END .................C=0.1, kernel=rbf;, score=0.490 total time=   8.9s\n",
            "[CV 2/2] END .................C=0.1, kernel=rbf;, score=0.490 total time=   9.6s\n",
            "[CV 1/2] END ................C=1, kernel=linear;, score=0.489 total time=  15.1s\n",
            "[CV 2/2] END ................C=1, kernel=linear;, score=0.489 total time=  15.2s\n",
            "[CV 1/2] END ...................C=1, kernel=rbf;, score=0.507 total time=  13.3s\n",
            "[CV 2/2] END ...................C=1, kernel=rbf;, score=0.503 total time=  10.4s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.04      0.07       994\n",
            "           1       0.51      0.98      0.67      2359\n",
            "           2       0.52      0.07      0.13      1443\n",
            "\n",
            "    accuracy                           0.51      4796\n",
            "   macro avg       0.60      0.36      0.29      4796\n",
            "weighted avg       0.57      0.51      0.38      4796\n",
            "\n",
            "Accuracy: 0.51\n",
            "SHAP summary plot not available for SVC model.\n",
            "Training LogisticRegression...\n",
            "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
            "[CV 1/2] END .............................C=0.1;, score=0.489 total time=   0.1s\n",
            "[CV 2/2] END .............................C=0.1;, score=0.490 total time=   0.1s\n",
            "[CV 1/2] END ...............................C=1;, score=0.491 total time=   0.2s\n",
            "[CV 2/2] END ...............................C=1;, score=0.490 total time=   0.2s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.01      0.01       994\n",
            "           1       0.49      1.00      0.66      2359\n",
            "           2       0.56      0.01      0.01      1443\n",
            "\n",
            "    accuracy                           0.49      4796\n",
            "   macro avg       0.51      0.34      0.23      4796\n",
            "weighted avg       0.51      0.49      0.33      4796\n",
            "\n",
            "Accuracy: 0.49\n",
            "SHAP summary plot not available for LogisticRegression model.\n",
            "Training RandomForest...\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END ...................n_estimators=10;, score=0.724 total time=   0.5s\n",
            "[CV 2/2] END ...................n_estimators=10;, score=0.731 total time=   0.5s\n",
            "[CV 1/2] END ...................n_estimators=50;, score=0.764 total time=   2.3s\n",
            "[CV 2/2] END ...................n_estimators=50;, score=0.768 total time=   2.3s\n",
            "[CV 1/2] END ..................n_estimators=100;, score=0.766 total time=   5.2s\n",
            "[CV 2/2] END ..................n_estimators=100;, score=0.773 total time=   4.6s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#!pip install ta\n",
        "#!pip install shap\n",
        "#\n",
        "#!pip install xgboost\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "import ta\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_and_prepare_data(folder_path):\n",
        "    files = glob.glob(f\"{folder_path}/*.csv\")[:20]\n",
        "    dataframes = []\n",
        "\n",
        "    for file in files:\n",
        "        df = pd.read_csv(file)\n",
        "        df['Close Horizon'] = df['Close'].shift(-20)\n",
        "        df['Horizon Return'] = (df['Close Horizon'] - df['Close']) / df['Close']\n",
        "        df['Label'] = df['Horizon Return'].apply(lambda x: 2 if x > 0.05 else (0 if x < -0.05 else 1))\n",
        "\n",
        "        df['SMA 20'] = ta.trend.sma_indicator(df['Close'], window=20)\n",
        "        df['EMA 20'] = ta.trend.ema_indicator(df['Close'], window=20)\n",
        "        df['RSI 14'] = ta.momentum.rsi(df['Close'], window=14)\n",
        "        df['MACD'] = ta.trend.macd(df['Close'])\n",
        "        df['MACD Signal'] = ta.trend.macd_signal(df['Close'])\n",
        "        df['Bollinger High'] = ta.volatility.bollinger_hband(df['Close'])\n",
        "        df['Bollinger Low'] = ta.volatility.bollinger_lband(df['Close'])\n",
        "        df['Rolling Volatility 20'] = df['Close'].rolling(window=20).std()\n",
        "        df['ROC 10'] = ta.momentum.roc(df['Close'], window=10)\n",
        "\n",
        "        df.dropna(inplace=True)\n",
        "        dataframes.append(df)\n",
        "\n",
        "    full_data = pd.concat(dataframes, ignore_index=True)\n",
        "    return full_data\n",
        "\n",
        "def preprocess_data(df):\n",
        "    X = df.drop(columns=['Label', 'Close Horizon', 'Horizon Return'])\n",
        "    Y = df['Label']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "def train_and_evaluate_model(model, param_grid, X_train, X_test, Y_train, Y_test):\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=2, scoring='accuracy', verbose = 3 )\n",
        "    grid_search.fit(X_train, Y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    Y_pred = best_model.predict(X_test)\n",
        "    print(classification_report(Y_test, Y_pred))\n",
        "    print(f\"Accuracy: {accuracy_score(Y_test, Y_pred):.2f}\")\n",
        "\n",
        "    # Check if the model is tree-based before using TreeExplainer\n",
        "    if isinstance(best_model, (RandomForestClassifier, XGBClassifier)):\n",
        "        explainer = shap.TreeExplainer(best_model)\n",
        "        shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "        #shap.summary_plot(X_test,  shap_values)\n",
        "        shap.summary_plot(shap_values, X_test, plot_type=\"dot\", max_display=3) # Changed to handle both single and multi-output models\n",
        "    else:\n",
        "        print(f\"SHAP summary plot not available for {type(best_model).__name__} model.\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "def main():\n",
        "    desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "    project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "    folder_path = os.path.join(project_path, \"data\", \"companies\")\n",
        "    df = load_and_prepare_data(folder_path)\n",
        "    df = df.drop(columns=[\"Date\"])\n",
        "    X_train, X_test, Y_train, Y_test = preprocess_data(df)\n",
        "\n",
        "    models = {\n",
        "        \"XGBoost\": (XGBClassifier(), {'n_estimators': [10, 40], 'learning_rate': [0.01, 0.1]}),\n",
        "        \"KNN\": (KNeighborsClassifier(), {'n_neighbors': [3, 5]}),\n",
        "        \"SVM\": (SVC(), {'C': [0.1, 1], 'kernel': ['linear', 'rbf']}),\n",
        "        \"LogisticRegression\": (LogisticRegression(), {'C': [0.1, 1]}),\n",
        "                \"RandomForest\": (RandomForestClassifier(), {'n_estimators': [10, 50, 100]}),\n",
        "\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for name, (model, params) in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        results[name] = train_and_evaluate_model(model, params, X_train, X_test, Y_train, Y_test)\n",
        "\n",
        "\n",
        "main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a6e151",
      "metadata": {
        "id": "43a6e151"
      },
      "source": [
        "# TP4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0ae71e",
      "metadata": {
        "id": "fe0ae71e"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "##############################################################################\n",
        "# 1) Dictionnaire des meilleurs hyperparamètres par entreprise\n",
        "##############################################################################\n",
        "best_params = {\n",
        "    \"Tesla\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 3,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 0.8,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 5,\n",
        "            'min_samples_leaf': 2,\n",
        "            'min_samples_split': 5,\n",
        "            'n_estimators': 50,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 9,\n",
        "            'p': 1,\n",
        "            'weights': 'uniform'\n",
        "        }\n",
        "    },\n",
        "    \"Samsung\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 1,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 5,\n",
        "            'min_samples_leaf': 1,\n",
        "            'min_samples_split': 5,\n",
        "            'n_estimators': 50,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 3,\n",
        "            'p': 2,\n",
        "            'weights': 'distance'\n",
        "        }\n",
        "    },\n",
        "    \"Tencent\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 0.8,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 1,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 10,\n",
        "            'min_samples_leaf': 2,\n",
        "            'min_samples_split': 2,\n",
        "            'n_estimators': 50,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 9,\n",
        "            'p': 2,\n",
        "            'weights': 'uniform'\n",
        "        }\n",
        "    },\n",
        "    \"Alibaba\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 7,\n",
        "            'min_child_weight': 3,\n",
        "            'n_estimators': 200,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': None,\n",
        "            'min_samples_leaf': 1,\n",
        "            'min_samples_split': 2,\n",
        "            'n_estimators': 200,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 3,\n",
        "            'p': 2,\n",
        "            'weights': 'distance'\n",
        "        }\n",
        "    },\n",
        "    \"Sony\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 3,\n",
        "            'n_estimators': 50,\n",
        "            'subsample': 0.8,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 3,\n",
        "            'min_samples_leaf': 1,\n",
        "            'min_samples_split': 2,\n",
        "            'n_estimators': 50,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 9,\n",
        "            'p': 2,\n",
        "            'weights': 'uniform'\n",
        "        }\n",
        "    },\n",
        "    \"Adobe\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 3,\n",
        "            'n_estimators': 50,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 5,\n",
        "            'min_samples_leaf': 2,\n",
        "            'min_samples_split': 5,\n",
        "            'n_estimators': 50,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 9,\n",
        "            'p': 2,\n",
        "            'weights': 'distance'\n",
        "        }\n",
        "    },\n",
        "    \"Johnson & Johnson\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 0.8,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 3,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 5,\n",
        "            'min_samples_leaf': 2,\n",
        "            'min_samples_split': 2,\n",
        "            'n_estimators': 100,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 9,\n",
        "            'p': 2,\n",
        "            'weights': 'uniform'\n",
        "        }\n",
        "    },\n",
        "    \"Pfizer\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 1,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': None,\n",
        "            'min_samples_leaf': 2,\n",
        "            'min_samples_split': 5,\n",
        "            'n_estimators': 100,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 7,\n",
        "            'p': 2,\n",
        "            'weights': 'distance'\n",
        "        }\n",
        "    },\n",
        "    \"Louis Vuitton (LVMH)\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 3,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 5,\n",
        "            'min_samples_leaf': 2,\n",
        "            'min_samples_split': 5,\n",
        "            'n_estimators': 100,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 9,\n",
        "            'p': 2,\n",
        "            'weights': 'uniform'\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "##############################################################################\n",
        "# 2) Fonctions utilitaires : Chargement, préparation des features\n",
        "##############################################################################\n",
        "def load_close_data(file_path):\n",
        "    \"\"\"Charge un CSV contenant au moins la colonne 'Close'.\n",
        "       Trie par date si la colonne 'Date' existe.\n",
        "       Retourne un DataFrame avec index=Date (si dispo) et colonne 'Close'.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df = df.sort_values('Date').set_index('Date')\n",
        "\n",
        "    if 'Close' not in df.columns:\n",
        "        raise ValueError(f\"Le fichier {file_path} ne contient pas la colonne 'Close'.\")\n",
        "\n",
        "    return df[['Close']]\n",
        "\n",
        "def create_target_features(df, n=30):\n",
        "    \"\"\"Reçoit un array df de shape (m,1).\n",
        "       Retourne X, Y avec une fenêtre glissante de n=30.\n",
        "    \"\"\"\n",
        "    x, y = [], []\n",
        "    for i in range(n, df.shape[0]):\n",
        "        x.append(df[i-n:i, 0])\n",
        "        y.append(df[i, 0])\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "def prepare_data_for_regression(file_path, n_days=30, test_ratio=0.2):\n",
        "    \"\"\"1) Charge le CSV, récupère 'Close'\n",
        "       2) MinMaxScaler\n",
        "       3) Crée X, Y avec create_target_features\n",
        "       4) Split train/test (split temporel)\n",
        "       5) Retourne X_train, Y_train, X_test, Y_test, scaler, close_vals\n",
        "    \"\"\"\n",
        "    df_close = load_close_data(file_path)\n",
        "    close_vals = df_close.values  # shape (m,1)\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    close_scaled = scaler.fit_transform(close_vals)\n",
        "\n",
        "    X, Y = create_target_features(close_scaled, n=n_days)\n",
        "\n",
        "    split_index = int((1 - test_ratio) * len(X))\n",
        "    X_train, Y_train = X[:split_index], Y[:split_index]\n",
        "    X_test,  Y_test  = X[split_index:], Y[split_index:]\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test, scaler, close_vals\n",
        "\n",
        "##############################################################################\n",
        "# 3) Fonctions de régression (avec best params) + régression linéaire simple\n",
        "##############################################################################\n",
        "def build_xgb(params):\n",
        "    \"\"\"Construit un XGBRegressor avec les paramètres donnés.\"\"\"\n",
        "    return XGBRegressor(**params)\n",
        "\n",
        "def build_rf(params):\n",
        "    \"\"\"Construit un RandomForestRegressor avec les paramètres donnés.\"\"\"\n",
        "    return RandomForestRegressor(**params)\n",
        "\n",
        "def build_knn(params):\n",
        "    \"\"\"Construit un KNeighborsRegressor avec les paramètres donnés.\"\"\"\n",
        "    return KNeighborsRegressor(**params)\n",
        "\n",
        "def build_linear():\n",
        "    \"\"\"Régression linéaire, sans hyperparamètres.\"\"\"\n",
        "    return LinearRegression()\n",
        "\n",
        "def train_and_predict(model, X_train, Y_train, X_test, Y_test):\n",
        "    \"\"\"Entraîne le modèle et renvoie (pred, mse, rmse) sur X_test, Y_test.\"\"\"\n",
        "    model.fit(X_train, Y_train)\n",
        "    pred = model.predict(X_test)\n",
        "\n",
        "    mse  = mean_squared_error(Y_test, pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return pred, mse, rmse\n",
        "\n",
        "##############################################################################\n",
        "# 4) Pipeline : pour une entreprise, on charge, on entraîne, on affiche\n",
        "##############################################################################\n",
        "def run_for_company(company_name, file_path, n_days=30):\n",
        "    \"\"\"Charge les données, entraîne XGB, RF, KNN (avec best params),\n",
        "       plus LinearRegression, calcule MSE/RMSE, et affiche la courbe.\n",
        "    \"\"\"\n",
        "    # Récupération des hyperparamètres depuis le dictionnaire\n",
        "    params_xgb = best_params[company_name][\"XGBoost\"]\n",
        "    params_rf  = best_params[company_name][\"RandomForest\"]\n",
        "    params_knn = best_params[company_name][\"KNN\"]\n",
        "\n",
        "    X_train, Y_train, X_test, Y_test, scaler, close_vals = \\\n",
        "        prepare_data_for_regression(file_path, n_days=n_days, test_ratio=0.2)\n",
        "\n",
        "    # Construction des 4 modèles\n",
        "    model_xgb = build_xgb(params_xgb)\n",
        "    model_rf  = build_rf(params_rf)\n",
        "    model_knn = build_knn(params_knn)\n",
        "    model_lr  = build_linear()\n",
        "\n",
        "    # Entraînement et prédictions (sur l'échelle normalisée)\n",
        "    preds_scaled = {}\n",
        "\n",
        "    preds_scaled[\"XGBoost\"], mse_xgb, rmse_xgb = train_and_predict(model_xgb, X_train, Y_train, X_test, Y_test)\n",
        "    preds_scaled[\"RandomForest\"], mse_rf, rmse_rf = train_and_predict(model_rf, X_train, Y_train, X_test, Y_test)\n",
        "    preds_scaled[\"KNN\"], mse_knn, rmse_knn = train_and_predict(model_knn, X_train, Y_train, X_test, Y_test)\n",
        "    preds_scaled[\"Linear\"], mse_lr, rmse_lr = train_and_predict(model_lr, X_train, Y_train, X_test, Y_test)\n",
        "\n",
        "    # Inversion du scaling\n",
        "    Y_test_true = scaler.inverse_transform(Y_test.reshape(-1,1)).ravel()\n",
        "\n",
        "    results = []\n",
        "    preds_true = {}\n",
        "\n",
        "    for model_name, y_scaled in preds_scaled.items():\n",
        "        y_true = scaler.inverse_transform(y_scaled.reshape(-1,1)).ravel()\n",
        "        preds_true[model_name] = y_true\n",
        "\n",
        "        # MSE/RMSE sur l'échelle inversée\n",
        "        mse_inv = mean_squared_error(Y_test_true, y_true)\n",
        "        rmse_inv = np.sqrt(mse_inv)\n",
        "\n",
        "        # Récupération du MSE/RMSE normalisé (pour l'instant on les recalcule)\n",
        "        mse_norm = mean_squared_error(Y_test, preds_scaled[model_name])\n",
        "        rmse_norm = np.sqrt(mse_norm)\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": model_name,\n",
        "            \"MSE_norm\": mse_norm,\n",
        "            \"RMSE_norm\": rmse_norm,\n",
        "            \"MSE_inversed\": mse_inv,\n",
        "            \"RMSE_inversed\": rmse_inv\n",
        "        })\n",
        "\n",
        "    df_res = pd.DataFrame(results)\n",
        "\n",
        "    # Plot\n",
        "    real_values = close_vals.ravel()\n",
        "    test_start_index = len(close_vals) - len(Y_test_true)\n",
        "    x_axis_pred = range(test_start_index, test_start_index + len(Y_test_true))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(real_values, label=\"Vraies valeurs (Close)\", color='black')\n",
        "    for model_name, y_pred in preds_true.items():\n",
        "        plt.plot(x_axis_pred, y_pred, label=f\"{model_name} Pred\")\n",
        "    plt.title(f\"Prédictions pour {company_name}\")\n",
        "    plt.xlabel(\"Index temporel\")\n",
        "    plt.ylabel(\"Prix de clôture\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n=== Résultats pour {company_name} ===\")\n",
        "    print(df_res)\n",
        "    return df_res\n",
        "\n",
        "##############################################################################\n",
        "# 5) Programme principal : on boucle sur la liste des entreprises\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "# 5) Programme principal : on boucle sur la liste des entreprises\n",
        "##############################################################################\n",
        "def main():\n",
        "    companies = {\n",
        "        \"Tesla\": \"TSLA\",\n",
        "        \"Samsung\": \"005930.KS\",\n",
        "        \"Tencent\": \"TCEHY\",\n",
        "        \"Alibaba\": \"BABA\",\n",
        "        \"Sony\": \"SONY\",\n",
        "        \"Adobe\": \"ADBE\",\n",
        "        \"Johnson & Johnson\": \"JNJ\",\n",
        "        \"Pfizer\": \"PFE\",\n",
        "        \"Louis Vuitton (LVMH)\": \"MC.PA\",\n",
        "    }\n",
        "\n",
        "    data_folder = \"Companies historical data\"  # Dossier où se trouvent les CSV\n",
        "    desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "    project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "    data_folder = os.path.join(project_path, \"data\", \"companies\")\n",
        "\n",
        "    all_results = [] # Initialize the list here\n",
        "\n",
        "    for company_name, symbol in companies.items():\n",
        "        # Nom de fichier (ex: \"Tesla_historical_data.csv\")\n",
        "\n",
        "        csv_file = f\"{company_name.replace(' ', '_')}.csv\"\n",
        "        file_path = os.path.join(data_folder, csv_file)\n",
        "\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Fichier introuvable : {file_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n--- {company_name} ({symbol}) ---\")\n",
        "        df_res = run_for_company(company_name, file_path, n_days=30)\n",
        "\n",
        "        # Ajout de la colonne \"Company\" pour identification\n",
        "        df_res[\"Company\"] = company_name\n",
        "        all_results.append(df_res)\n",
        "\n",
        "    # Concaténation finale\n",
        "    if len(all_results) > 0:\n",
        "        final_df = pd.concat(all_results, ignore_index=True)\n",
        "        print(\"\\n=== Résumé final ===\")\n",
        "        print(final_df)\n",
        "\n",
        "        # Optionnel : sauvegarder en CSV\n",
        "        # final_df.to_csv(\"final_results_companies.csv\", index=False)\n",
        "    else:\n",
        "        print(\"Aucun fichier traité.\")\n",
        "\n",
        "main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22440e2",
      "metadata": {
        "id": "f22440e2"
      },
      "source": [
        "# TP5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad79bdde",
      "metadata": {
        "id": "ad79bdde"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"TP5_Neural_Networks_Corrected.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1Api-Kt2nHTzn3hwhq7QmpI4-jNbzgRCx\n",
        "\n",
        "# TP5 – Réseaux de Neurones pour la Prédiction de Valeurs Boursières\n",
        "\"\"\"\n",
        "\n",
        "companies = {\n",
        "            \"Louis Vuitton (LVMH)\": \"MC.PA\",\n",
        "            \"Tesla\": \"TSLA\",\n",
        "        \"Samsung\": \"005930.KS\",\n",
        "        \"Tencent\": \"TCEHY\",\n",
        "        \"Alibaba\": \"BABA\",\n",
        "        \"Sony\": \"SONY\",\n",
        "        \"Adobe\": \"ADBE\",\n",
        "        \"Johnson & Johnson\": \"JNJ\",\n",
        "        \"Pfizer\": \"PFE\",\n",
        "}\n",
        "tickers = list(companies.values())\n",
        "len(tickers)\n",
        "\n",
        "# 📥 Téléchargement des données AAPL\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "df = yf.download(\"AAPL\", start=\"2020-01-01\", end=\"2023-12-31\")[[\"Close\"]]\n",
        "df = df.reset_index()\n",
        "\n",
        "df.columns.name = None\n",
        "df.index.name = None\n",
        "df.head()\n",
        "\n",
        "# 📊 Préparation des données\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def prepare_data(df, feature_col=\"Close\", window_size=30):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(df[[feature_col]])\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(scaled_data)):\n",
        "        X.append(scaled_data[i-window_size:i])\n",
        "        y.append(scaled_data[i])\n",
        "    return np.array(X), np.array(y), scaler\n",
        "\n",
        "X, y, scaler = prepare_data(df)\n",
        "X.shape, y.shape\n",
        "\n",
        "# 🔀 Séparation des données\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "X_train.shape, X_test.shape\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# ✅ MLP optimisé\n",
        "def build_mlp_model(input_shape, hidden_dims=[128, 64, 32], activation='relu', dropout_rate=0.4, optimizer='adam'):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten(input_shape=input_shape))\n",
        "    for dim in hidden_dims:\n",
        "        model.add(tf.keras.layers.Dense(dim, activation=activation, kernel_initializer='glorot_uniform'))\n",
        "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# ✅ RNN optimisé\n",
        "def build_rnn_model(input_shape, units=100, dropout_rate=0.4, optimizer='adam'):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.SimpleRNN(units, activation='tanh', input_shape=input_shape, return_sequences=False,\n",
        "                                        kernel_initializer='glorot_uniform'))\n",
        "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# ✅ LSTM optimisé\n",
        "def build_lstm_model(input_shape, units=100, dropout_rate=0.4, optimizer='adam'):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.LSTM(units, activation='tanh', input_shape=input_shape, return_sequences=False,\n",
        "                                   kernel_initializer='glorot_uniform'))\n",
        "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# 🚀 Entraînement générique\n",
        "def train_model(model_type, X_train, y_train, input_shape, **kwargs):\n",
        "\n",
        "    if model_type == \"MLP\":\n",
        "        model = build_mlp_model(input_shape, **kwargs)\n",
        "    elif model_type == \"RNN\":\n",
        "        model = build_rnn_model(input_shape, **kwargs)\n",
        "    elif model_type == \"LSTM\":\n",
        "        model = build_lstm_model(input_shape, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(\"Type de modèle inconnu.\")\n",
        "    model.fit(X_train, y_train, epochs=kwargs.get(\"epochs\", 100), batch_size=kwargs.get(\"batch_size\", 32), verbose=kwargs.get(\"verbose\", 0) )\n",
        "    return model\n",
        "\n",
        "def predict(model, X_test, y_test, scaler, model_type):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
        "    y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\n",
        "\n",
        "    print(f\"{model_type} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
        "    print(\"\\n10 premières valeurs prédites vs vraies :\")\n",
        "    for p, t in zip(y_pred_rescaled[:10], y_test_rescaled[:10]):\n",
        "        print(f\"Prédit : {p[0]:.2f} | Réel : {t[0]:.2f}\")\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(y_test_rescaled, label=\"Réel\")\n",
        "    plt.plot(y_pred_rescaled, label=\"Prédit\")\n",
        "    plt.title(f\"{model_type} - Prédictions vs Réel\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return mae, rmse\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "print(project_path)\n",
        "\n",
        "for company in companies.keys():\n",
        "    print(f'Training For Company {company}')\n",
        "    tf_model_path = os.path.join(project_path, f\"{company}_models_results\")\n",
        "    os.makedirs(tf_model_path, exist_ok=True)\n",
        "    # 1. Chargement des données\n",
        "    df = yf.download(companies[company], start=\"2020-01-01\", end=\"2023-12-31\")[[\"Close\"]]\n",
        "    df = df.reset_index()\n",
        "\n",
        "    # 2. Préparation\n",
        "    X, y, scaler = prepare_data(df, window_size=30)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # 3. Tests des modèles\n",
        "    results = []\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "\n",
        "    # MLP\n",
        "    model_mlp = train_model(\"MLP\", X_train, y_train, X_train.shape[1:] )\n",
        "    mae_mlp, rmse_mlp = predict(model_mlp, X_test, y_test, scaler, \"MLP\")\n",
        "    results.append([\"MLP\", mae_mlp, rmse_mlp])\n",
        "    model_mlp.save(os.path.join(tf_model_path, \"mlp_model.keras\"))\n",
        "\n",
        "    # RNN\n",
        "    model_rnn = train_model(\"RNN\", X_train, y_train, X_train.shape[1:] )\n",
        "    mae_rnn, rmse_rnn = predict(model_rnn, X_test, y_test, scaler, \"RNN\")\n",
        "    results.append([\"RNN\", mae_rnn, rmse_rnn])\n",
        "    model_rnn.save(os.path.join(tf_model_path, \"rnn_model.keras\"))\n",
        "\n",
        "    # LSTM\n",
        "    model_lstm = train_model(\"LSTM\", X_train, y_train, X_train.shape[1:] )\n",
        "    mae_lstm, rmse_lstm = predict(model_lstm, X_test, y_test, scaler, \"LSTM\")\n",
        "    results.append([\"LSTM\", mae_lstm, rmse_lstm])\n",
        "    model_lstm.save(os.path.join(tf_model_path, \"lstm_model.keras\"))\n",
        "\n",
        "\n",
        "\n",
        "    # Résumé\n",
        "    results_df = pd.DataFrame(results, columns=[\"Modèle\", \"MAE\", \"RMSE\"])\n",
        "    results_df.to_csv(os.path.join(tf_model_path, f\"{company}_results.csv\"), index=False)\n",
        "    print(f\"\\n📊 Résumé des performances pour {company}:\")\n",
        "    print(results_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a2ea60f",
      "metadata": {
        "id": "3a2ea60f"
      },
      "source": [
        "# TP6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"TP6_pratique_DS.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1ZJ6A7C41e2YVifCRWNO4bOjFPG3Ww0ig\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def load_existing_news(company_name):\n",
        "    \"\"\"Charge les news existantes depuis un fichier JSON\"\"\"\n",
        "    filename = f\"{company_name.lower().replace(' ', '_')}_news.json\"\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            return json.load(file)\n",
        "    return {}\n",
        "\n",
        "def save_news(company_name, news_dict):\n",
        "    \"\"\"Sauvegarde les news dans un fichier JSON\"\"\"\n",
        "    filename = f\"{company_name.lower().replace(' ', '_')}_news.json\"\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        json.dump(news_dict, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "def get_news_by_date(company_name, api_key):\n",
        "    \"\"\"Scrape les news récentes et sauvegarde les nouvelles\"\"\"\n",
        "    url = 'https://newsapi.org/v2/everything'\n",
        "    last_day = datetime.today().strftime('%Y-%m-%d')\n",
        "    first_day = (datetime.today() - timedelta(days=10)).strftime('%Y-%m-%d')\n",
        "\n",
        "    params = {\n",
        "        \"q\": company_name,\n",
        "        \"apiKey\": api_key,\n",
        "        \"language\": \"en\",\n",
        "        \"pageSize\": 100,\n",
        "        \"from\": first_day,\n",
        "        \"to\": last_day,\n",
        "        \"sources\": 'financial-post,the-wall-street-journal,bloomberg,the-washington-post,australian-financial-review'\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    news_dict = load_existing_news(company_name)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        articles = response.json().get('articles', [])\n",
        "        for article in articles:\n",
        "            title = article.get('title', '')\n",
        "            description = article.get('description', '')\n",
        "            published_at = article.get('publishedAt', '')\n",
        "            source_name = article.get('source', {}).get('name', '')\n",
        "\n",
        "            if company_name.lower() in title.lower() or company_name.lower() in description.lower():\n",
        "                date = published_at.split(\"T\")[0]\n",
        "                article_data = {\n",
        "                    'title': title,\n",
        "                    'description': description,\n",
        "                    'source': source_name,\n",
        "                    'date': date\n",
        "                }\n",
        "\n",
        "                # Vérifie si l'article existe déjà\n",
        "                if date not in news_dict:\n",
        "                    news_dict[date] = []\n",
        "\n",
        "                if not any(existing['title'] == title for existing in news_dict[date]):\n",
        "                    news_dict[date].append(article_data)\n",
        "\n",
        "        # Sauvegarde des nouvelles données\n",
        "        save_news(company_name, news_dict)\n",
        "        print(f\"✔️ Articles mis à jour pour {company_name}\")\n",
        "    else:\n",
        "        print(f\"❌ Erreur lors de la requête : {response.status_code} - {response.text}\")\n",
        "\n",
        "api_key = \"5fb037a89b2b401cb25a416eca7e5ade\"\n",
        "list_company_name = [\"Tesla\", \"Apple\"]\n",
        "\n",
        "# Corrected definition of all_companies list\n",
        "all_companies = [\n",
        "\"Apple\",\n",
        "\"Microsoft\",\n",
        "\"Amazon\",\n",
        "\"Alphabet\",\n",
        "\"Meta\",\n",
        "\"Tesla\",\n",
        "\"NVIDIA\",\n",
        "\"Samsung\",\n",
        "\"Tencent\",\n",
        "\"Alibaba\",\n",
        "\"IBM\",\n",
        "\"Intel\",\n",
        "\"Oracle\",\n",
        "\"Sony\",\n",
        "\"Adobe\",\n",
        "\"Netflix\",\n",
        "\"AMD\",\n",
        "\"Qualcomm\",\n",
        "\"Cisco\",\n",
        "\"JP Morgan\",\n",
        "\"Goldman Sachs\",\n",
        "\"Visa\",\n",
        "\"Johnson & Johnson\",\n",
        "\"Pfizer\",\n",
        "\"ExxonMobil\",\n",
        "\"ASML\",\n",
        "\"SAP\",\n",
        "\"Siemens\",\n",
        "\"Louis Vuitton (LVMH)\",\n",
        "\"TotalEnergies\",\n",
        "\"Shell\",\n",
        "\"Baidu\",\n",
        "\"JD.com\",\n",
        "\"BYD\",\n",
        "\"ICBC\",\n",
        "\"Toyota\",\n",
        "\"SoftBank\",\n",
        "\"Nintendo\",\n",
        "\"Hyundai\",\n",
        "\"Reliance Industries\",\n",
        "\"Tata Consultancy Services\"\n",
        "]\n",
        "\n",
        "for company_name in list_company_name :\n",
        "  get_news_by_date(company_name, api_key)\n",
        "\n",
        "for company_name in all_companies:\n",
        "  get_news_by_date(company_name, api_key)"
      ],
      "metadata": {
        "id": "XxoHBVHiADAA"
      },
      "id": "XxoHBVHiADAA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "acd74b53",
      "metadata": {
        "id": "acd74b53"
      },
      "source": [
        "# TP 7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade datasets"
      ],
      "metadata": {
        "id": "93wCVjrQTYNN"
      },
      "id": "93wCVjrQTYNN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea86cc7f",
      "metadata": {
        "id": "ea86cc7f"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, concatenate_datasets, DatasetDict, ClassLabel\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
        "from zipfile import ZipFile\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# It's recommended to run this in a separate Colab cell if you need to upgrade datasets\n",
        "# !pip install --upgrade datasets\n",
        "\n",
        "def load_financial_dataset(test_size=0.2, seed=42):\n",
        "    ds1 = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
        "    # ds1 expected schema: \"text\", \"label\" (already ClassLabel or int)\n",
        "\n",
        "    ds2 = load_dataset(\"nickmuchi/financial-classification\")\n",
        "    # ds2 schema from Hub: \"sentence\", \"labels\" (ClassLabel or int)\n",
        "    # Error implies for user: \"text\", \"labels\" or \"text\", \"label\"\n",
        "\n",
        "    train_ds1 = ds1[\"train\"]\n",
        "    train_ds2 = ds2[\"train\"]\n",
        "\n",
        "    # Standardize column names for ds2 to be \"text\" and \"label\"\n",
        "\n",
        "    # 1. Handle label column in ds2\n",
        "    # Ensure it's named \"label\"\n",
        "    if \"labels\" in train_ds2.column_names:\n",
        "        if \"labels\" != \"label\": # Only rename if the name is actually \"labels\"\n",
        "            train_ds2 = train_ds2.rename_column(\"labels\", \"label\")\n",
        "    elif \"label\" not in train_ds2.column_names:\n",
        "        raise ValueError(f\"Label column ('labels' or 'label') not found in ds2 (nickmuchi/financial-classification). Columns: {train_ds2.column_names}\")\n",
        "    # Now ds2 has a \"label\" column.\n",
        "\n",
        "    # 2. Handle text column in ds2\n",
        "    # Ensure it's named \"text\"\n",
        "    if \"sentence\" in train_ds2.column_names:\n",
        "        if \"sentence\" != \"text\": # Only rename if the name is actually \"sentence\"\n",
        "            train_ds2 = train_ds2.rename_column(\"sentence\", \"text\")\n",
        "    elif \"text\" not in train_ds2.column_names:\n",
        "        # This case means neither \"sentence\" nor \"text\" was found.\n",
        "        raise ValueError(f\"Text column ('sentence' or 'text') not found in ds2 (nickmuchi/financial-classification). Columns: {train_ds2.column_names}\")\n",
        "\n",
        "    common_class_label = ClassLabel(num_classes=3, names=['neutral', 'positive', 'negative'])\n",
        "\n",
        "\n",
        "    if train_ds1.features[\"label\"].dtype != common_class_label.dtype or \\\n",
        "       str(train_ds1.features[\"label\"]) != str(common_class_label): # More robust check\n",
        "        train_ds1 = train_ds1.cast_column(\"label\", common_class_label)\n",
        "\n",
        "    if train_ds2.features[\"label\"].dtype != common_class_label.dtype or \\\n",
        "       str(train_ds2.features[\"label\"]) != str(common_class_label):\n",
        "        train_ds2 = train_ds2.cast_column(\"label\", common_class_label)\n",
        "\n",
        "    train_ds1 = train_ds1.filter(lambda example: example['label'] >= 0 and example['label'] < common_class_label.num_classes)\n",
        "    train_ds2 = train_ds2.filter(lambda example: example['label'] >= 0 and example['label'] < common_class_label.num_classes)\n",
        "\n",
        "    full_train = concatenate_datasets([train_ds1, train_ds2])\n",
        "\n",
        "    # Stratified split\n",
        "    split = full_train.train_test_split(test_size=test_size, seed=seed, stratify_by_column=\"label\")\n",
        "\n",
        "    return DatasetDict({\n",
        "        \"train\": split[\"train\"],\n",
        "        \"test\": split[\"test\"]\n",
        "    })\n",
        "\n",
        "\n",
        "dataset = load_financial_dataset()\n",
        "print(dataset)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "MAX_TOKEN_LEN = 512 # Max sequence length for BERT-like models\n",
        "\n",
        "def train_model(model_name, dataset_dict, batch_size, num_epochs):\n",
        "\n",
        "    #desktop = \".\"\n",
        "    desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "    project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "    os.makedirs(project_path, exist_ok=True)\n",
        "\n",
        "    model_save_path = os.path.join(project_path, \"ProsusAI_finbert_results\")\n",
        "    os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "    output_dir = os.path.join(project_path, f\"{model_name.replace('/', '_')}_results\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    # Ensure num_labels matches the dataset (3 for neutral, positive, negative)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_TOKEN_LEN)\n",
        "\n",
        "    # Remove original text column after tokenization to save memory/disk\n",
        "    tokenized_train = dataset_dict[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "    tokenized_test = dataset_dict[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "    # Data collator will dynamically pad the batched examples\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_epochs,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        save_total_limit=2,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        gradient_checkpointing=True,\n",
        "        report_to=\"none\",\n",
        "        disable_tqdm=False,\n",
        "        logging_first_step=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_test,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    train_result = trainer.train()\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    print(\"Evaluating model...\")\n",
        "    eval_result = trainer.evaluate()\n",
        "    print(\"Évaluation finale :\", eval_result)\n",
        "\n",
        "    print(f\"Saving model to {model_save_path}...\")\n",
        "    trainer.save_model(model_save_path)\n",
        "    print(f\"Modèle sauvegardé dans {model_save_path}\")\n",
        "\n",
        "    del model\n",
        "    del trainer\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return eval_result, train_result\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "eval_metrics, train_history = train_model(\"ProsusAI/finbert\", dataset, batch_size=16, num_epochs=3)\n",
        "print(f\"Training time : {time.time() - start_time:.2f} seconds\")\n",
        "print(\"Final Evaluation Metrics:\", eval_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "88Rxkz96U91C"
      },
      "id": "88Rxkz96U91C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "295e1ec6",
      "metadata": {
        "id": "295e1ec6"
      },
      "source": [
        "# TP 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf3e824a",
      "metadata": {
        "id": "bf3e824a"
      },
      "outputs": [],
      "source": [
        "\n",
        "#! pip install transformers\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import pytz\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from matplotlib.lines import Line2D\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "model_name = \"ProsusAI_finbert\"\n",
        "desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "model_save_path = os.path.join(project_path, \"ProsusAI_finbert_results\")\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "output_dir = os.path.join(project_path, f\"{model_name.replace('/', '_')}_results\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def get_text_timestamps(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        news_data = json.load(file)\n",
        "\n",
        "    texts = []\n",
        "    timestamps = []\n",
        "\n",
        "    for date, articles in news_data.items():\n",
        "        for item in articles:\n",
        "            try:\n",
        "                utc_time = datetime.strptime(item['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "                ny_time = utc_time.replace(tzinfo=pytz.utc).astimezone(pytz.timezone('America/New_York'))\n",
        "                rounded_time = ny_time.replace(minute=0, second=0, microsecond=0)\n",
        "\n",
        "                title = item.get('title', '')\n",
        "                description = item.get('description', '')\n",
        "                full_text = f\"{title} {description}\".strip()\n",
        "\n",
        "                if full_text:\n",
        "                    texts.append(full_text)\n",
        "                    timestamps.append(rounded_time)\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur sur un article : {e}\")\n",
        "                continue\n",
        "\n",
        "    return texts, timestamps\n",
        "\n",
        "# put all json files in a folder called json files with the json files inside\n",
        "# The folder should be in the same directory as main file\n",
        "# -- json_files -- Nintendo_news.json\n",
        "#   |            |  ....\n",
        "#   |           -- ,,,news.json\n",
        "# -- main.py\n",
        "get_text_timestamps('json_files/Nintendo_news.json')\n",
        "\n",
        "with open('json_files/Nintendo_news.json', 'r', encoding='utf-8') as file:\n",
        "    news_data = json.load(file)\n",
        "\n",
        "def get_sentiment(model_path, texts):\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "    sentiments = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            sentiment = torch.argmax(logits, dim=1).item()\n",
        "            sentiments.append(sentiment)\n",
        "\n",
        "    return sentiments\n",
        "\n",
        "def align_timestamps(timestamps):\n",
        "    aligned = []\n",
        "    for ts in timestamps:\n",
        "        ts = pd.Timestamp(ts).tz_localize(None)  # Pour uniformiser avec Pandas\n",
        "        market_open = ts.replace(hour=9, minute=30, second=0, microsecond=0)\n",
        "        market_close = ts.replace(hour=15, minute=0, second=0, microsecond=0)\n",
        "\n",
        "        if market_open <= ts < market_close:\n",
        "            aligned.append(ts)\n",
        "        elif market_close <= ts < ts.replace(hour=23, minute=59, second=59):\n",
        "            aligned.append(market_close)\n",
        "        else:\n",
        "            # Entre minuit et 9h30 => mapper à 15h la veille\n",
        "            prev_day = (ts - pd.Timedelta(days=1)).replace(hour=15, minute=0, second=0, microsecond=0)\n",
        "            aligned.append(prev_day)\n",
        "    return aligned\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "ts = datetime(2025, 3, 15, 8, 45, tzinfo=pytz.timezone(\"America/New_York\"))\n",
        "aligned_ts = align_timestamps([ts])[0]\n",
        "print(aligned_ts)\n",
        "# ➜ Doit afficher : 2025-03-14 15:00:00\n",
        "def plot_comparison(df, sentiment_a, sentiment_b, timestamps, title_a, title_b):\n",
        "    aligned_ts = align_timestamps(timestamps)\n",
        "\n",
        "    # Rendre les timestamps tz-naive pour éviter l'erreur de comparaison\n",
        "    df['Datetime'] = df['Datetime'].dt.tz_localize(None)\n",
        "    aligned_ts = [ts.replace(tzinfo=None) for ts in aligned_ts]\n",
        "\n",
        "    # Grouper les sentiments par timestamp\n",
        "    def group_sentiments(ts_list, sentiments):\n",
        "        grouped = defaultdict(list)\n",
        "        for ts, sent in zip(ts_list, sentiments):\n",
        "            grouped[ts].append(sent)\n",
        "        return grouped\n",
        "\n",
        "    grouped_a = group_sentiments(aligned_ts, sentiment_a)\n",
        "    grouped_b = group_sentiments(aligned_ts, sentiment_b)\n",
        "\n",
        "    # Pour chaque timestamp, calculer le nombre de news par sentiment\n",
        "    def get_points(grouped):\n",
        "        points = []\n",
        "        for ts, sents in grouped.items():\n",
        "            for i, sent in enumerate(sents):\n",
        "                points.append((ts, sent, i))\n",
        "        return points\n",
        "\n",
        "    points_a = get_points(grouped_a)\n",
        "    points_b = get_points(grouped_b)\n",
        "\n",
        "    sentiment_colors = {2: 'green', 1: 'gold', 0: 'red'}\n",
        "    sentiment_labels = {2: 'Positif', 1: 'Neutre', 0: 'Négatif'}\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=True)\n",
        "\n",
        "    for ax, points, title in zip(axes, [points_a, points_b], [title_a, title_b]):\n",
        "        # Tracer la courbe de prix\n",
        "        ax.plot(df['Datetime'], df['Close'], color='blue', label='Prix action')\n",
        "        # Superposer les news\n",
        "        for ts, sent, idx in points:\n",
        "            # Trouver le timestamp le plus proche dans df['Datetime']\n",
        "            nearest_ts = df['Datetime'].iloc[(df['Datetime'] - ts).abs().argsort()[0]]\n",
        "            y = df.loc[df['Datetime'] == nearest_ts, 'Close']\n",
        "            if not y.empty:\n",
        "                y_val = y.values[0] + (idx - 0.5) * 0.5  # Décalage vertical\n",
        "                ax.scatter(nearest_ts, y_val, color=sentiment_colors[sent], s=80, edgecolor='black', zorder=5)\n",
        "        # Ajouter le titre du subplot\n",
        "        ax.set_title(title)\n",
        "\n",
        "    axes[0].set_ylabel('Prix de clôture')\n",
        "\n",
        "    # Légende personnalisée\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='blue', lw=2, label='Prix action'),\n",
        "        Line2D([0], [0], marker='o', color='w', label='Positif', markerfacecolor='green', markersize=10, markeredgecolor='black'),\n",
        "        Line2D([0], [0], marker='o', color='w', label='Neutre', markerfacecolor='gold', markersize=10, markeredgecolor='black'),\n",
        "        Line2D([0], [0], marker='o', color='w', label='Négatif', markerfacecolor='red', markersize=10, markeredgecolor='black')\n",
        "    ]\n",
        "    axes[1].legend(handles=legend_elements, loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_company_sentiment(ticker, news_path, model_a_path, model_b_path, title_a, title_b):\n",
        "    print(f\"\\nTraitement de {ticker}...\")\n",
        "\n",
        "    # 1. Récupérer les news\n",
        "    texts, timestamps = get_text_timestamps(news_path)\n",
        "\n",
        "    # 2. Prédire les sentiments avec les deux modèles\n",
        "    sentiments_a = get_sentiment(model_a_path, texts)\n",
        "    sentiments_b = get_sentiment(model_b_path, texts)\n",
        "\n",
        "    # 3. Récupérer les données de marché\n",
        "    df_stock = get_stock_data(ticker)\n",
        "\n",
        "    # 4. Afficher les graphiques\n",
        "    plot_comparison(df_stock, sentiments_a, sentiments_b, timestamps, title_a, title_b)\n",
        "\n",
        "def get_stock_data(ticker, start_date=\"2025-01-01\"):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    df = stock.history(start=start_date, interval=\"60m\")\n",
        "    df = df.reset_index()\n",
        "    df.rename(columns={\"Datetime\": \"Datetime\", \"Close\": \"Close\"}, inplace=True)\n",
        "    return df[[\"Datetime\", \"Close\"]]\n",
        "\n",
        "companies = {\n",
        "        \"Amazon\": \"AMZN\",\n",
        "\n",
        "            \"Louis Vuitton (LVMH)\": \"MC.PA\",\n",
        "            \"Tesla\": \"TSLA\",\n",
        "        \"Samsung\": \"005930.KS\",\n",
        "        \"Tencent\": \"TCEHY\",\n",
        "        \"Alibaba\": \"BABA\",\n",
        "        \"Sony\": \"SONY\",\n",
        "        \"Adobe\": \"ADBE\",\n",
        "        \"Johnson & Johnson\": \"JNJ\",\n",
        "        \"Pfizer\": \"PFE\",\n",
        "}\n",
        "\n",
        "for company_name , ticker in companies.items():\n",
        "    adjusted_company_name = company_name.replace(\" \", \"_\")\n",
        "    news_path = f\"json_files/{adjusted_company_name}_news.json\"\n",
        "    analyze_company_sentiment(\n",
        "        ticker=ticker,\n",
        "        news_path=news_path,\n",
        "        model_a_path=\"ProsusAI/finbert\",              # FinBERT original\n",
        "        model_b_path=model_save_path,      # Ton modèle fine-tuné\n",
        "        title_a=\"FinBERT (original)\",\n",
        "        title_b=\"FinBERT (fine-tuné)\"\n",
        "    )\n",
        "    print(f\"Traitement de {company_name} terminé.\")\n",
        "    print(\"-\" * 50)\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import pytz\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from matplotlib.lines import Line2D\n",
        "import numpy as np\n",
        "\n",
        "# Configuration\n",
        "model_name = \"ProsusAI_finbert\"\n",
        "desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "\n",
        "# Paths for models and outputs\n",
        "model_save_path = os.path.join(project_path, \"ProsusAI_finbert_results\")\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# New directory for TP8 plots\n",
        "tp8_output_dir = os.path.join(project_path, \"tp8_plots\")\n",
        "os.makedirs(tp8_output_dir, exist_ok=True)\n",
        "\n",
        "def get_text_timestamps(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        news_data = json.load(file)\n",
        "\n",
        "    texts = []\n",
        "    timestamps = []\n",
        "\n",
        "    for date, articles in news_data.items():\n",
        "        for item in articles:\n",
        "            try:\n",
        "                utc_time = datetime.strptime(item['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "                ny_time = utc_time.replace(tzinfo=pytz.utc).astimezone(pytz.timezone('America/New_York'))\n",
        "                rounded_time = ny_time.replace(minute=0, second=0, microsecond=0)\n",
        "\n",
        "                title = item.get('title', '')\n",
        "                description = item.get('description', '')\n",
        "                full_text = f\"{title} {description}\".strip()\n",
        "\n",
        "                if full_text:\n",
        "                    texts.append(full_text)\n",
        "                    timestamps.append(rounded_time)\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur sur un article : {e}\")\n",
        "                continue\n",
        "\n",
        "    return texts, timestamps\n",
        "\n",
        "\n",
        "def get_sentiment(model_path, texts):\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "    sentiments = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            sentiment = torch.argmax(outputs.logits, dim=1).item()\n",
        "            sentiments.append(sentiment)\n",
        "    return sentiments\n",
        "\n",
        "\n",
        "def align_timestamps(timestamps):\n",
        "    aligned = []\n",
        "    for ts in timestamps:\n",
        "        ts = pd.Timestamp(ts).tz_localize(None)\n",
        "        market_open = ts.replace(hour=9, minute=30)\n",
        "        market_close = ts.replace(hour=15, minute=0)\n",
        "        if market_open <= ts < market_close:\n",
        "            aligned.append(ts)\n",
        "        elif market_close <= ts:\n",
        "            aligned.append(market_close)\n",
        "        else:\n",
        "            prev_day = (ts - pd.Timedelta(days=1)).replace(hour=15, minute=0)\n",
        "            aligned.append(prev_day)\n",
        "    return aligned\n",
        "\n",
        "\n",
        "def get_stock_data(ticker, start_date=\"2025-01-01\"):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    df = stock.history(start=start_date, interval=\"60m\").reset_index()\n",
        "    return df[[\"Datetime\", \"Close\"]]\n",
        "\n",
        "\n",
        "def plot_comparison(df, sentiment_a, sentiment_b, timestamps, title_a, title_b, company_name, output_dir):\n",
        "    aligned_ts = align_timestamps(timestamps)\n",
        "    # Ensure tz-naive\n",
        "    df['Datetime'] = df['Datetime'].dt.tz_localize(None)\n",
        "    aligned_naive = [ts.replace(tzinfo=None) for ts in aligned_ts]\n",
        "\n",
        "    def group_sentiments(ts_list, sents):\n",
        "        grouped = defaultdict(list)\n",
        "        for ts, sent in zip(ts_list, sents):\n",
        "            grouped[ts].append(sent)\n",
        "        return grouped\n",
        "\n",
        "    grouped_a = group_sentiments(aligned_naive, sentiment_a)\n",
        "    grouped_b = group_sentiments(aligned_naive, sentiment_b)\n",
        "\n",
        "    def get_points(grouped):\n",
        "        pts = []\n",
        "        for ts, s_list in grouped.items():\n",
        "            for idx, sent in enumerate(s_list):\n",
        "                pts.append((ts, sent, idx))\n",
        "        return pts\n",
        "\n",
        "    points_a = get_points(grouped_a)\n",
        "    points_b = get_points(grouped_b)\n",
        "\n",
        "    sentiment_colors = {2:'green',1:'gold',0:'red'}\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18,6), sharey=True)\n",
        "    for ax, pts, title in zip(axes, [points_a, points_b],[title_a,title_b]):\n",
        "        ax.plot(df['Datetime'], df['Close'], label='Prix action')\n",
        "        for ts, sent, idx in pts:\n",
        "            nearest = df['Datetime'].iloc[(df['Datetime']-ts).abs().argsort()[0]]\n",
        "            y = df.loc[df['Datetime']==nearest,'Close'].values\n",
        "            if y.size:\n",
        "                y_val = y[0] + (idx-0.5)*0.5\n",
        "                ax.scatter(nearest, y_val, color=sentiment_colors[sent], s=80, edgecolor='black', zorder=5)\n",
        "        ax.set_title(title)\n",
        "    axes[0].set_ylabel('Prix de clôture')\n",
        "\n",
        "    legend_elements = [\n",
        "        Line2D([0],[0],color='blue',lw=2,label='Prix action'),\n",
        "        Line2D([0],[0],marker='o',color='w',label='Positif',markerfacecolor='green',markersize=10,markeredgecolor='black'),\n",
        "        Line2D([0],[0],marker='o',color='w',label='Neutre',markerfacecolor='gold',markersize=10,markeredgecolor='black'),\n",
        "        Line2D([0],[0],marker='o',color='w',label='Négatif',markerfacecolor='red',markersize=10,markeredgecolor='black')\n",
        "    ]\n",
        "    axes[1].legend(handles=legend_elements, loc='upper left')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    save_path = os.path.join(output_dir, f\"{company_name.replace(' ','_')}_tp8.png\")\n",
        "    fig.savefig(save_path)\n",
        "    print(f\"📸 Plot saved to {save_path}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# Main loop for TP8\n",
        "companies = {\n",
        "    \"Amazon\": \"AMZN\",\n",
        "    \"Louis Vuitton (LVMH)\": \"MC.PA\",\n",
        "    \"Tesla\": \"TSLA\",\n",
        "    \"Samsung\": \"005930.KS\",\n",
        "    \"Tencent\": \"TCEHY\",\n",
        "    \"Alibaba\": \"BABA\",\n",
        "    \"Sony\": \"SONY\",\n",
        "    \"Adobe\": \"ADBE\",\n",
        "    \"Johnson & Johnson\": \"JNJ\",\n",
        "    \"Pfizer\": \"PFE\",\n",
        "}\n",
        "\n",
        "for company_name, ticker in companies.items():\n",
        "    adjusted = company_name.replace(' ', '_')\n",
        "    news_path = os.path.join('json_files', f\"{adjusted}_news.json\")\n",
        "    texts, timestamps = get_text_timestamps(news_path)\n",
        "    sents_a = get_sentiment('ProsusAI/finbert', texts)\n",
        "    sents_b = get_sentiment(model_save_path, texts)\n",
        "    df_stock = get_stock_data(ticker)\n",
        "\n",
        "    plot_comparison(\n",
        "        df_stock, sents_a, sents_b, timestamps,\n",
        "        \"FinBERT (original)\", \"FinBERT (fine-tuné)\",\n",
        "        company_name, tp8_output_dir\n",
        "    )\n",
        "    print(f\"Traitement de {company_name} terminé.\")\n"
      ],
      "metadata": {
        "id": "gjdC74Ojog3W"
      },
      "id": "gjdC74Ojog3W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls /root/Desktop/Projet_Final_DS/tp8_plots/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0mJmgMloh0U",
        "outputId": "592a9b77-95bc-4d0e-939c-d167d9595449"
      },
      "id": "q0mJmgMloh0U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Adobe_tp8.png\t\t     'Louis_Vuitton_(LVMH)_tp8.png'   Tencent_tp8.png\n",
            " Alibaba_tp8.png\t      Pfizer_tp8.png\t\t      Tesla_tp8.png\n",
            " Amazon_tp8.png\t\t      Samsung_tp8.png\n",
            "'Johnson_&_Johnson_tp8.png'   Sony_tp8.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSC0MjOxpIKE"
      },
      "id": "jSC0MjOxpIKE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}