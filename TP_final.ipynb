{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "533f33ab",
      "metadata": {
        "id": "533f33ab"
      },
      "source": [
        "# TP1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3680627c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3680627c",
        "outputId": "16747611-af65-4b3b-fb5b-30aafb122a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ñ∂ Scraping des ratios financiers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [00:49<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ratios financiers sauvegard√©s dans /root/Desktop/Projet_Final_DS/data/ratios.csv\n",
            "‚ñ∂ D√©but du scraping des variations boursi√®res...\n",
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Apple -> /root/Desktop/Projet_Final_DS/data/companies/Apple.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Microsoft -> /root/Desktop/Projet_Final_DS/data/companies/Microsoft.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Amazon -> /root/Desktop/Projet_Final_DS/data/companies/Amazon.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Alphabet -> /root/Desktop/Projet_Final_DS/data/companies/Alphabet.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Meta -> /root/Desktop/Projet_Final_DS/data/companies/Meta.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Tesla -> /root/Desktop/Projet_Final_DS/data/companies/Tesla.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour NVIDIA -> /root/Desktop/Projet_Final_DS/data/companies/NVIDIA.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Samsung -> /root/Desktop/Projet_Final_DS/data/companies/Samsung.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Tencent -> /root/Desktop/Projet_Final_DS/data/companies/Tencent.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Alibaba -> /root/Desktop/Projet_Final_DS/data/companies/Alibaba.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour IBM -> /root/Desktop/Projet_Final_DS/data/companies/IBM.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Intel -> /root/Desktop/Projet_Final_DS/data/companies/Intel.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Oracle -> /root/Desktop/Projet_Final_DS/data/companies/Oracle.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Sony -> /root/Desktop/Projet_Final_DS/data/companies/Sony.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Adobe -> /root/Desktop/Projet_Final_DS/data/companies/Adobe.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Netflix -> /root/Desktop/Projet_Final_DS/data/companies/Netflix.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour AMD -> /root/Desktop/Projet_Final_DS/data/companies/AMD.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Qualcomm -> /root/Desktop/Projet_Final_DS/data/companies/Qualcomm.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Cisco -> /root/Desktop/Projet_Final_DS/data/companies/Cisco.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour JP Morgan -> /root/Desktop/Projet_Final_DS/data/companies/JP_Morgan.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Goldman Sachs -> /root/Desktop/Projet_Final_DS/data/companies/Goldman_Sachs.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Visa -> /root/Desktop/Projet_Final_DS/data/companies/Visa.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Johnson & Johnson -> /root/Desktop/Projet_Final_DS/data/companies/Johnson_&_Johnson.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Pfizer -> /root/Desktop/Projet_Final_DS/data/companies/Pfizer.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour ExxonMobil -> /root/Desktop/Projet_Final_DS/data/companies/ExxonMobil.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour ASML -> /root/Desktop/Projet_Final_DS/data/companies/ASML.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour SAP -> /root/Desktop/Projet_Final_DS/data/companies/SAP.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Siemens -> /root/Desktop/Projet_Final_DS/data/companies/Siemens.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Louis Vuitton (LVMH) -> /root/Desktop/Projet_Final_DS/data/companies/Louis_Vuitton_(LVMH).csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour TotalEnergies -> /root/Desktop/Projet_Final_DS/data/companies/TotalEnergies.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Shell -> /root/Desktop/Projet_Final_DS/data/companies/Shell.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Baidu -> /root/Desktop/Projet_Final_DS/data/companies/Baidu.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour JD.com -> /root/Desktop/Projet_Final_DS/data/companies/JD.com.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour BYD -> /root/Desktop/Projet_Final_DS/data/companies/BYD.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour ICBC -> /root/Desktop/Projet_Final_DS/data/companies/ICBC.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Toyota -> /root/Desktop/Projet_Final_DS/data/companies/Toyota.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour SoftBank -> /root/Desktop/Projet_Final_DS/data/companies/SoftBank.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Nintendo -> /root/Desktop/Projet_Final_DS/data/companies/Nintendo.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Hyundai -> /root/Desktop/Projet_Final_DS/data/companies/Hyundai.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Reliance Industries -> /root/Desktop/Projet_Final_DS/data/companies/Reliance_Industries.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Donn√©es export√©es pour Tata Consultancy Services -> /root/Desktop/Projet_Final_DS/data/companies/Tata_Consultancy_Services.csv\n",
            "\n",
            "üì¢ *R√©capitulatif final*\n",
            "‚úÖ Nombre de fichiers export√©s avec succ√®s : 41/41\n",
            "\n",
            "üéâ Toutes les entreprises ont √©t√© r√©cup√©r√©es avec succ√®s !\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import time\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "companies = {\n",
        "    \"Apple\": \"AAPL\",\n",
        "    \"Microsoft\": \"MSFT\",\n",
        "    \"Amazon\": \"AMZN\",\n",
        "    \"Alphabet\": \"GOOGL\",\n",
        "    \"Meta\": \"META\",\n",
        "    \"Tesla\": \"TSLA\",\n",
        "    \"NVIDIA\": \"NVDA\",\n",
        "    \"Samsung\": \"005930.KS\",\n",
        "    \"Tencent\": \"TCEHY\",\n",
        "    \"Alibaba\": \"BABA\",\n",
        "    \"IBM\": \"IBM\",\n",
        "    \"Intel\": \"INTC\",\n",
        "    \"Oracle\": \"ORCL\",\n",
        "    \"Sony\": \"SONY\",\n",
        "    \"Adobe\": \"ADBE\",\n",
        "    \"Netflix\": \"NFLX\",\n",
        "    \"AMD\": \"AMD\",\n",
        "    \"Qualcomm\": \"QCOM\",\n",
        "    \"Cisco\": \"CSCO\",\n",
        "    \"JP Morgan\": \"JPM\",\n",
        "    \"Goldman Sachs\": \"GS\",\n",
        "    \"Visa\": \"V\",\n",
        "    \"Johnson & Johnson\": \"JNJ\",\n",
        "    \"Pfizer\": \"PFE\",\n",
        "    \"ExxonMobil\": \"XOM\",\n",
        "    \"ASML\": \"ASML.AS\",\n",
        "    \"SAP\": \"SAP.DE\",\n",
        "    \"Siemens\": \"SIE.DE\",\n",
        "    \"Louis Vuitton (LVMH)\": \"MC.PA\",\n",
        "    \"TotalEnergies\": \"TTE.PA\",\n",
        "    \"Shell\": \"SHEL.L\",\n",
        "    \"Baidu\": \"BIDU\",\n",
        "    \"JD.com\": \"JD\",\n",
        "    \"BYD\": \"BYDDY\",\n",
        "    \"ICBC\": \"1398.HK\",\n",
        "    \"Toyota\": \"TM\",\n",
        "    \"SoftBank\": \"9984.T\",\n",
        "    \"Nintendo\": \"NTDOY\",\n",
        "    \"Hyundai\": \"HYMTF\",\n",
        "    \"Reliance Industries\": \"RELIANCE.NS\",\n",
        "    \"Tata Consultancy Services\": \"TCS.NS\"\n",
        "}\n",
        "\n",
        "def safe_get_ticker_info(symbol, retries=1, delay=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            return yf.Ticker(symbol).info\n",
        "        except Exception as e:\n",
        "            if \"Rate limited\" in str(e):\n",
        "                print(f\"‚è≥ Rate limited pour {symbol}. Nouvelle tentative dans {delay} secondes...\")\n",
        "                time.sleep(delay)\n",
        "                delay *= 2\n",
        "            else:\n",
        "                raise e\n",
        "    raise Exception(f\"‚ùå √âchec apr√®s {retries} tentatives pour {symbol}\")\n",
        "\n",
        "def scrape_financial_ratios(output_path):\n",
        "    print(\"‚ñ∂ Scraping des ratios financiers...\")\n",
        "    ratios = {\n",
        "        \"forwardPE\": [], \"beta\": [], \"priceToBook\": [], \"priceToSales\": [],\n",
        "        \"dividendYield\": [], \"trailingEps\": [], \"debtToEquity\": [],\n",
        "        \"currentRatio\": [], \"quickRatio\": [], \"returnOnEquity\": [],\n",
        "        \"returnOnAssets\": [], \"operatingMargins\": [], \"profitMargins\": []\n",
        "    }\n",
        "\n",
        "    company_names = []\n",
        "\n",
        "    for company_name, symbol in tqdm(companies.items()):\n",
        "        try:\n",
        "            info = safe_get_ticker_info(symbol)\n",
        "            company_names.append(company_name)\n",
        "            for ratio_key in ratios.keys():\n",
        "                value = info.get(ratio_key, None)\n",
        "                ratios[ratio_key].append(value)\n",
        "            time.sleep(1)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur pour {company_name}: {e}\")\n",
        "            for ratio_key in ratios:\n",
        "                ratios[ratio_key].append(None)\n",
        "            company_names.append(company_name)\n",
        "\n",
        "    df_ratios = pd.DataFrame(ratios, index=company_names)\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    df_ratios.to_csv(output_path, encoding='utf-8')\n",
        "    print(f\"‚úÖ Ratios financiers sauvegard√©s dans {output_path}\")\n",
        "\n",
        "def fetch_stock_variations(company_name, symbol, start_date, end_date, output_folder):\n",
        "    try:\n",
        "        df = yf.download(symbol, start=start_date, end=end_date)\n",
        "        df.columns = df.columns.droplevel(1)\n",
        "        df.columns.name = None\n",
        "        df.index.name = None\n",
        "        df = df[[\"Close\"]]\n",
        "        df = df.reset_index()\n",
        "        df.rename(columns={\"index\": \"Date\"}, inplace=True)\n",
        "\n",
        "        if df.empty:\n",
        "            raise ValueError(\"DataFrame vide ‚Äì possible rate limit ou symbole invalide.\")\n",
        "\n",
        "        #df = df[['Close']].copy()\n",
        "\n",
        "        #  Transforme l'index (Date) en colonne explicite\n",
        "\n",
        "        # Conversion s√©curis√©e de 'Close' en float\n",
        "        df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
        "        df.dropna(subset=['Close'], inplace=True)\n",
        "\n",
        "        df['Next_Day_Close'] = df['Close'].shift(-1)\n",
        "        df['Daily_Return'] = df['Close'].pct_change()\n",
        "        df.dropna(inplace=True)\n",
        "\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "        file_path = os.path.join(output_folder, f\"{company_name.replace(' ', '_')}.csv\")\n",
        "        df.to_csv(file_path)\n",
        "        time.sleep(1)\n",
        "        return True, company_name, file_path\n",
        "    except Exception as e:\n",
        "        return False, company_name, str(e)\n",
        "\n",
        "def main():\n",
        "    desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "    project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "    os.makedirs(project_path, exist_ok=True)\n",
        "\n",
        "    start_date = (datetime.today() - timedelta(days=365*5)).strftime(\"%Y-%m-%d\")\n",
        "    end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "    output_folder = os.path.join(project_path, \"data\", \"companies\")\n",
        "    ratios_path = os.path.join(project_path, \"data\", \"ratios.csv\")\n",
        "\n",
        "    # Partie 1 : Ratios financiers\n",
        "    scrape_financial_ratios(ratios_path)\n",
        "\n",
        "    # Partie 2 : Variations journali√®res\n",
        "    success_count = 0\n",
        "    failed_companies = []\n",
        "\n",
        "    print(\"‚ñ∂ D√©but du scraping des variations boursi√®res...\")\n",
        "    for company_name, symbol in companies.items():\n",
        "        success, name, result = fetch_stock_variations(company_name, symbol, start_date, end_date, output_folder)\n",
        "        if success:\n",
        "            success_count += 1\n",
        "            print(f\"‚úÖ Donn√©es export√©es pour {name} -> {result}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Erreur pour {name} : {result}\")\n",
        "            failed_companies.append(name)\n",
        "\n",
        "    print(\"\\nüì¢ *R√©capitulatif final*\")\n",
        "    print(f\"‚úÖ Nombre de fichiers export√©s avec succ√®s : {success_count}/{len(companies)}\")\n",
        "\n",
        "    if failed_companies:\n",
        "        print(\"\\n‚ùó Entreprises non r√©cup√©r√©es :\")\n",
        "        for failed in failed_companies:\n",
        "            print(f\"  - {failed}\")\n",
        "    else:\n",
        "        print(\"\\nüéâ Toutes les entreprises ont √©t√© r√©cup√©r√©es avec succ√®s !\")\n",
        "#\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aae431b",
      "metadata": {
        "id": "6aae431b"
      },
      "source": [
        "# TP2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"json_files/AMD_news.json\" ) as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "ZhINxQevz4Lh"
      },
      "id": "ZhINxQevz4Lh",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3XnsPN_QzgDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f8d512-6805-4f67-c28e-a76361f791ce"
      },
      "id": "3XnsPN_QzgDA",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls /root/Desktop/Projet_Final_DS/data/companies/"
      ],
      "metadata": {
        "id": "ILY1l0Rz0S4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ce6c2e-cd92-48f5-bafe-863c56a359fb"
      },
      "id": "ILY1l0Rz0S4V",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Adobe.csv\t     ICBC.csv\t\t\t Reliance_Industries.csv\n",
            " Alibaba.csv\t     Intel.csv\t\t\t Samsung.csv\n",
            " Alphabet.csv\t     JD.com.csv\t\t\t SAP.csv\n",
            " Amazon.csv\t    'Johnson_&_Johnson.csv'\t Shell.csv\n",
            " AMD.csv\t     JP_Morgan.csv\t\t Siemens.csv\n",
            " Apple.csv\t    'Louis_Vuitton_(LVMH).csv'\t SoftBank.csv\n",
            " ASML.csv\t     Meta.csv\t\t\t Sony.csv\n",
            " Baidu.csv\t     Microsoft.csv\t\t Tata_Consultancy_Services.csv\n",
            " BYD.csv\t     Netflix.csv\t\t Tencent.csv\n",
            " Cisco.csv\t     Nintendo.csv\t\t Tesla.csv\n",
            " ExxonMobil.csv      NVIDIA.csv\t\t\t TotalEnergies.csv\n",
            " Goldman_Sachs.csv   Oracle.csv\t\t\t Toyota.csv\n",
            " Hyundai.csv\t     Pfizer.csv\t\t\t Visa.csv\n",
            " IBM.csv\t     Qualcomm.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1704e958",
      "metadata": {
        "id": "1704e958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75893451-a1fc-4aca-fa64-3568ec524564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-1a4e88e0a15d>:25: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_returns = pd.DataFrame(all_returns).fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä R√©sultats pour : Financial Profile\n",
            "KMeans Silhouette Score      : 0.359\n",
            "Hierarchical Silhouette Score: 0.367\n",
            "DBSCAN Silhouette Score      : 0.436\n",
            "‚úÖ Meilleur algo : DBSCAN\n",
            "‚úÖ Donn√©es + labels sauvegard√©s : /root/Desktop/Projet_Final_DS/outputs/clustering/financial_profile_labels.csv\n",
            "üì∏ TSNE sauvegard√© : financial_profile_kmeans_tsne.png\n",
            "üì∏ TSNE sauvegard√© : financial_profile_hierarchical_tsne.png\n",
            "üì∏ TSNE sauvegard√© : financial_profile_dbscan_tsne.png\n",
            "\n",
            "üìä R√©sultats pour : Risk Profile\n",
            "KMeans Silhouette Score      : 0.526\n",
            "Hierarchical Silhouette Score: 0.571\n",
            "DBSCAN Silhouette Score      : 0.767\n",
            "‚úÖ Meilleur algo : DBSCAN\n",
            "‚úÖ Donn√©es + labels sauvegard√©s : /root/Desktop/Projet_Final_DS/outputs/clustering/risk_profile_labels.csv\n",
            "üì∏ TSNE sauvegard√© : risk_profile_kmeans_tsne.png\n",
            "üì∏ TSNE sauvegard√© : risk_profile_hierarchical_tsne.png\n",
            "üì∏ TSNE sauvegard√© : risk_profile_dbscan_tsne.png\n",
            "\n",
            "üìä R√©sultats pour : Daily Returns Correlation\n",
            "KMeans Silhouette Score      : 0.261\n",
            "Hierarchical Silhouette Score: 0.286\n",
            "DBSCAN Silhouette Score      : 0.257\n",
            "‚úÖ Meilleur algo : Hierarchical\n",
            "‚úÖ Donn√©es + labels sauvegard√©s : /root/Desktop/Projet_Final_DS/outputs/clustering/daily_returns_correlation_labels.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/scipy/cluster/hierarchy.py:796: ClusterWarning: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
            "  return linkage(y, method='ward', metric='euclidean')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì∏ TSNE sauvegard√© : daily_returns_correlation_kmeans_tsne.png\n",
            "üì∏ TSNE sauvegard√© : daily_returns_correlation_hierarchical_tsne.png\n",
            "üì∏ TSNE sauvegard√© : daily_returns_correlation_dbscan_tsne.png\n",
            "üìÑ R√©sum√© des scores sauvegard√© dans : /root/Desktop/Projet_Final_DS/outputs/clustering/clustering_results.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def preprocess_for_financial_clustering(filepath, selected_columns):\n",
        "    df = pd.read_csv(filepath, index_col=0)\n",
        "    df_clean = df[selected_columns].dropna()\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = scaler.fit_transform(df_clean)\n",
        "    return pd.DataFrame(data_scaled, index=df_clean.index, columns=df_clean.columns), df_clean\n",
        "\n",
        "def prepare_daily_returns(folder_path):\n",
        "    all_returns = {}\n",
        "    for filepath in glob.glob(os.path.join(folder_path, \"*.csv\")):\n",
        "        company = os.path.basename(filepath).replace(\".csv\", \"\").replace(\"_\", \" \")\n",
        "        df = pd.read_csv(filepath)\n",
        "        if \"Daily_Return\" in df.columns:\n",
        "            all_returns[company] = df[\"Daily_Return\"]\n",
        "    df_returns = pd.DataFrame(all_returns).fillna(method='ffill').fillna(method='bfill')\n",
        "    return df_returns.corr()\n",
        "\n",
        "def do_kmeans(data, n_clusters):\n",
        "    model = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    labels = model.fit_predict(data)\n",
        "    return labels, silhouette_score(data, labels)\n",
        "\n",
        "def do_hierarchical(data, n_clusters):\n",
        "    model = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    labels = model.fit_predict(data)\n",
        "    return labels, silhouette_score(data, labels)\n",
        "\n",
        "def do_dbscan(data, eps=1, min_samples=2):\n",
        "    model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels = model.fit_predict(data)\n",
        "    try:\n",
        "        score = silhouette_score(data, labels) if len(set(labels)) > 1 else -1\n",
        "    except:\n",
        "        score = -1\n",
        "    return labels, score\n",
        "\n",
        "def save_labels_with_data(df_data, labels_dict, dataset_name, output_path):\n",
        "    df_out = df_data.copy()\n",
        "    for algo, labels in labels_dict.items():\n",
        "        df_out[algo + \"_Cluster\"] = labels\n",
        "    filename = dataset_name.lower().replace(\" \", \"_\") + \"_labels.csv\"\n",
        "    full_path = os.path.join(output_path, filename)\n",
        "    df_out.to_csv(full_path)\n",
        "    print(f\"‚úÖ Donn√©es + labels sauvegard√©s : {full_path}\")\n",
        "\n",
        "def plot_tsne(data_scaled, labels_dict, dataset_name, output_path):\n",
        "    tsne = TSNE(n_components=2, random_state=42, init=\"random\", perplexity=5)\n",
        "    tsne_results = tsne.fit_transform(data_scaled)\n",
        "\n",
        "    for algo, clusters in labels_dict.items():\n",
        "        df_tsne = pd.DataFrame(tsne_results, columns=[\"TSNE1\", \"TSNE2\"])\n",
        "        df_tsne[\"Cluster\"] = clusters\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        unique_clusters = np.unique(clusters)\n",
        "        colors = plt.colormaps.get_cmap(\"tab10\")\n",
        "\n",
        "        for i, cluster in enumerate(unique_clusters):\n",
        "            subset = df_tsne[df_tsne[\"Cluster\"] == cluster]\n",
        "            plt.scatter(subset[\"TSNE1\"], subset[\"TSNE2\"],\n",
        "                        label=f\"Cluster {cluster}\",\n",
        "                        color=colors(i % 10),\n",
        "                        alpha=0.7)\n",
        "\n",
        "        plt.xlabel(\"TSNE 1\")\n",
        "        plt.ylabel(\"TSNE 2\")\n",
        "        plt.title(f\"TSNE - {dataset_name} - {algo}\")\n",
        "        plt.legend()\n",
        "        filename = f\"{dataset_name.lower().replace(' ', '_')}_{algo.lower()}_tsne.png\"\n",
        "        plt.savefig(os.path.join(output_path, filename))\n",
        "        plt.close()\n",
        "        print(f\"üì∏ TSNE sauvegard√© : {filename}\")\n",
        "\n",
        "def main():\n",
        "    desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "    project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "    ratios_path = os.path.join(project_path, \"data\", \"ratios.csv\")\n",
        "    returns_folder = os.path.join(project_path, \"data\", \"companies\")\n",
        "    output_path = os.path.join(project_path, \"outputs\", \"clustering\")\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    selected_columns_finance = [\"forwardPE\", \"beta\", \"priceToBook\", \"returnOnEquity\"]\n",
        "    selected_columns_risk = [\"debtToEquity\", \"currentRatio\", \"quickRatio\"]\n",
        "\n",
        "    data_finance_scaled, data_finance_raw = preprocess_for_financial_clustering(ratios_path, selected_columns_finance)\n",
        "    data_risk_scaled, data_risk_raw = preprocess_for_financial_clustering(ratios_path, selected_columns_risk)\n",
        "    data_returns = prepare_daily_returns(returns_folder)\n",
        "    data_returns_dist = 1 - data_returns\n",
        "\n",
        "    datasets = {\n",
        "        \"Financial Profile\": (data_finance_scaled, data_finance_raw),\n",
        "        \"Risk Profile\": (data_risk_scaled, data_risk_raw),\n",
        "        \"Daily Returns Correlation\": (data_returns_dist, data_returns)\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for name, (data_scaled, data_original) in datasets.items():\n",
        "        print(f\"\\nüìä R√©sultats pour : {name}\")\n",
        "        try:\n",
        "            labels_km, score_km = do_kmeans(data_scaled, n_clusters=5)\n",
        "            labels_hc, score_hc = do_hierarchical(data_scaled, n_clusters=5)\n",
        "            labels_db, score_db = do_dbscan(data_scaled, eps=1, min_samples=2)\n",
        "\n",
        "            all_scores = {\n",
        "                \"KMeans\": score_km,\n",
        "                \"Hierarchical\": score_hc,\n",
        "                \"DBSCAN\": score_db\n",
        "            }\n",
        "            best_algo = max(all_scores, key=all_scores.get)\n",
        "\n",
        "            print(f\"KMeans Silhouette Score      : {score_km:.3f}\")\n",
        "            print(f\"Hierarchical Silhouette Score: {score_hc:.3f}\")\n",
        "            print(f\"DBSCAN Silhouette Score      : {score_db:.3f}\")\n",
        "            print(f\"‚úÖ Meilleur algo : {best_algo}\")\n",
        "\n",
        "            results.append({\n",
        "                \"Dataset\": name,\n",
        "                \"KMeans Silhouette\": score_km,\n",
        "                \"Hierarchical Silhouette\": score_hc,\n",
        "                \"DBSCAN Silhouette\": score_db,\n",
        "                \"Best Algorithm\": best_algo\n",
        "            })\n",
        "\n",
        "            labels_dict = {\n",
        "                \"KMeans\": labels_km,\n",
        "                \"Hierarchical\": labels_hc,\n",
        "                \"DBSCAN\": labels_db\n",
        "            }\n",
        "\n",
        "            save_labels_with_data(data_original, labels_dict, name, output_path)\n",
        "            plot_tsne(data_scaled, labels_dict, name, output_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur sur {name} : {e}\")\n",
        "            results.append({\n",
        "                \"Dataset\": name,\n",
        "                \"KMeans Silhouette\": None,\n",
        "                \"Hierarchical Silhouette\": None,\n",
        "                \"DBSCAN Silhouette\": None,\n",
        "                \"Best Algorithm\": f\"Erreur: {e}\"\n",
        "            })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_csv_path = os.path.join(output_path, \"clustering_results.csv\")\n",
        "    results_df.to_csv(results_csv_path, index=False)\n",
        "    print(f\"üìÑ R√©sum√© des scores sauvegard√© dans : {results_csv_path}\")\n",
        "\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf38040e",
      "metadata": {
        "id": "cf38040e"
      },
      "source": [
        "# TP 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ta"
      ],
      "metadata": {
        "id": "lh9ESMRL3U0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0850ab78-31c4-409b-f816-2c35dfbc694a"
      },
      "id": "lh9ESMRL3U0k",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=4a83198c94183d5b2f1391edb619d7d37f425a65554cef39d0745fd106a0ac4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n"
      ],
      "metadata": {
        "id": "lIox0IqO7PTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a91a5760-4dca-4dc8-c0a7-41645fdfadd2"
      },
      "id": "lIox0IqO7PTR",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4cc3793",
      "metadata": {
        "id": "b4cc3793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "809e0c37-738e-4a69-b77a-d672d57afd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training XGBoost...\n",
            "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
            "[CV 1/2] END learning_rate=0.01, n_estimators=10;, score=0.537 total time=   1.0s\n",
            "[CV 2/2] END learning_rate=0.01, n_estimators=10;, score=0.544 total time=   1.8s\n",
            "[CV 1/2] END learning_rate=0.01, n_estimators=40;, score=0.555 total time=   5.2s\n",
            "[CV 2/2] END learning_rate=0.01, n_estimators=40;, score=0.557 total time=   4.2s\n",
            "[CV 1/2] END learning_rate=0.1, n_estimators=10;, score=0.562 total time=   1.6s\n",
            "[CV 2/2] END learning_rate=0.1, n_estimators=10;, score=0.577 total time=   0.7s\n",
            "[CV 1/2] END learning_rate=0.1, n_estimators=40;, score=0.605 total time=   3.8s\n",
            "[CV 2/2] END learning_rate=0.1, n_estimators=40;, score=0.615 total time=   1.1s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.36      0.48       994\n",
            "           1       0.59      0.89      0.71      2359\n",
            "           2       0.71      0.38      0.50      1443\n",
            "\n",
            "    accuracy                           0.62      4796\n",
            "   macro avg       0.68      0.54      0.56      4796\n",
            "weighted avg       0.66      0.62      0.60      4796\n",
            "\n",
            "Accuracy: 0.62\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 550x340 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training KNN...\n",
            "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
            "[CV 1/2] END .....................n_neighbors=3;, score=0.488 total time=   1.5s\n",
            "[CV 2/2] END .....................n_neighbors=3;, score=0.478 total time=   1.6s\n",
            "[CV 1/2] END .....................n_neighbors=5;, score=0.493 total time=   1.8s\n",
            "[CV 2/2] END .....................n_neighbors=5;, score=0.495 total time=   0.9s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.42      0.40       994\n",
            "           1       0.57      0.69      0.62      2359\n",
            "           2       0.51      0.31      0.39      1443\n",
            "\n",
            "    accuracy                           0.52      4796\n",
            "   macro avg       0.49      0.47      0.47      4796\n",
            "weighted avg       0.51      0.52      0.51      4796\n",
            "\n",
            "Accuracy: 0.52\n",
            "SHAP summary plot not available for KNeighborsClassifier model.\n",
            "Training SVM...\n",
            "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
            "[CV 1/2] END ..............C=0.1, kernel=linear;, score=0.489 total time=   8.7s\n",
            "[CV 2/2] END ..............C=0.1, kernel=linear;, score=0.489 total time=   9.2s\n",
            "[CV 1/2] END .................C=0.1, kernel=rbf;, score=0.490 total time=   8.9s\n",
            "[CV 2/2] END .................C=0.1, kernel=rbf;, score=0.490 total time=   9.6s\n",
            "[CV 1/2] END ................C=1, kernel=linear;, score=0.489 total time=  15.1s\n",
            "[CV 2/2] END ................C=1, kernel=linear;, score=0.489 total time=  15.2s\n",
            "[CV 1/2] END ...................C=1, kernel=rbf;, score=0.507 total time=  13.3s\n",
            "[CV 2/2] END ...................C=1, kernel=rbf;, score=0.503 total time=  10.4s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.04      0.07       994\n",
            "           1       0.51      0.98      0.67      2359\n",
            "           2       0.52      0.07      0.13      1443\n",
            "\n",
            "    accuracy                           0.51      4796\n",
            "   macro avg       0.60      0.36      0.29      4796\n",
            "weighted avg       0.57      0.51      0.38      4796\n",
            "\n",
            "Accuracy: 0.51\n",
            "SHAP summary plot not available for SVC model.\n",
            "Training LogisticRegression...\n",
            "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
            "[CV 1/2] END .............................C=0.1;, score=0.489 total time=   0.1s\n",
            "[CV 2/2] END .............................C=0.1;, score=0.490 total time=   0.1s\n",
            "[CV 1/2] END ...............................C=1;, score=0.491 total time=   0.2s\n",
            "[CV 2/2] END ...............................C=1;, score=0.490 total time=   0.2s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.01      0.01       994\n",
            "           1       0.49      1.00      0.66      2359\n",
            "           2       0.56      0.01      0.01      1443\n",
            "\n",
            "    accuracy                           0.49      4796\n",
            "   macro avg       0.51      0.34      0.23      4796\n",
            "weighted avg       0.51      0.49      0.33      4796\n",
            "\n",
            "Accuracy: 0.49\n",
            "SHAP summary plot not available for LogisticRegression model.\n",
            "Training RandomForest...\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END ...................n_estimators=10;, score=0.724 total time=   0.5s\n",
            "[CV 2/2] END ...................n_estimators=10;, score=0.731 total time=   0.5s\n",
            "[CV 1/2] END ...................n_estimators=50;, score=0.764 total time=   2.3s\n",
            "[CV 2/2] END ...................n_estimators=50;, score=0.768 total time=   2.3s\n",
            "[CV 1/2] END ..................n_estimators=100;, score=0.766 total time=   5.2s\n",
            "[CV 2/2] END ..................n_estimators=100;, score=0.773 total time=   4.6s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#!pip install ta\n",
        "#!pip install shap\n",
        "#\n",
        "#!pip install xgboost\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "import ta\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_and_prepare_data(folder_path):\n",
        "    files = glob.glob(f\"{folder_path}/*.csv\")[:20]\n",
        "    dataframes = []\n",
        "\n",
        "    for file in files:\n",
        "        df = pd.read_csv(file)\n",
        "        df['Close Horizon'] = df['Close'].shift(-20)\n",
        "        df['Horizon Return'] = (df['Close Horizon'] - df['Close']) / df['Close']\n",
        "        df['Label'] = df['Horizon Return'].apply(lambda x: 2 if x > 0.05 else (0 if x < -0.05 else 1))\n",
        "\n",
        "        df['SMA 20'] = ta.trend.sma_indicator(df['Close'], window=20)\n",
        "        df['EMA 20'] = ta.trend.ema_indicator(df['Close'], window=20)\n",
        "        df['RSI 14'] = ta.momentum.rsi(df['Close'], window=14)\n",
        "        df['MACD'] = ta.trend.macd(df['Close'])\n",
        "        df['MACD Signal'] = ta.trend.macd_signal(df['Close'])\n",
        "        df['Bollinger High'] = ta.volatility.bollinger_hband(df['Close'])\n",
        "        df['Bollinger Low'] = ta.volatility.bollinger_lband(df['Close'])\n",
        "        df['Rolling Volatility 20'] = df['Close'].rolling(window=20).std()\n",
        "        df['ROC 10'] = ta.momentum.roc(df['Close'], window=10)\n",
        "\n",
        "        df.dropna(inplace=True)\n",
        "        dataframes.append(df)\n",
        "\n",
        "    full_data = pd.concat(dataframes, ignore_index=True)\n",
        "    return full_data\n",
        "\n",
        "def preprocess_data(df):\n",
        "    X = df.drop(columns=['Label', 'Close Horizon', 'Horizon Return'])\n",
        "    Y = df['Label']\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "def train_and_evaluate_model(model, param_grid, X_train, X_test, Y_train, Y_test):\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=2, scoring='accuracy', verbose = 3 )\n",
        "    grid_search.fit(X_train, Y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    Y_pred = best_model.predict(X_test)\n",
        "    print(classification_report(Y_test, Y_pred))\n",
        "    print(f\"Accuracy: {accuracy_score(Y_test, Y_pred):.2f}\")\n",
        "\n",
        "    # Check if the model is tree-based before using TreeExplainer\n",
        "    if isinstance(best_model, (RandomForestClassifier, XGBClassifier)):\n",
        "        explainer = shap.TreeExplainer(best_model)\n",
        "        shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "        #shap.summary_plot(X_test,  shap_values)\n",
        "        shap.summary_plot(shap_values, X_test, plot_type=\"dot\", max_display=3) # Changed to handle both single and multi-output models\n",
        "    else:\n",
        "        print(f\"SHAP summary plot not available for {type(best_model).__name__} model.\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "def main():\n",
        "    desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "    project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "    folder_path = os.path.join(project_path, \"data\", \"companies\")\n",
        "    df = load_and_prepare_data(folder_path)\n",
        "    df = df.drop(columns=[\"Date\"])\n",
        "    X_train, X_test, Y_train, Y_test = preprocess_data(df)\n",
        "\n",
        "    models = {\n",
        "        \"XGBoost\": (XGBClassifier(), {'n_estimators': [10, 40], 'learning_rate': [0.01, 0.1]}),\n",
        "        \"KNN\": (KNeighborsClassifier(), {'n_neighbors': [3, 5]}),\n",
        "        \"SVM\": (SVC(), {'C': [0.1, 1], 'kernel': ['linear', 'rbf']}),\n",
        "        \"LogisticRegression\": (LogisticRegression(), {'C': [0.1, 1]}),\n",
        "                \"RandomForest\": (RandomForestClassifier(), {'n_estimators': [10, 50, 100]}),\n",
        "\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for name, (model, params) in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        results[name] = train_and_evaluate_model(model, params, X_train, X_test, Y_train, Y_test)\n",
        "\n",
        "\n",
        "main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a6e151",
      "metadata": {
        "id": "43a6e151"
      },
      "source": [
        "# TP4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0ae71e",
      "metadata": {
        "id": "fe0ae71e"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "##############################################################################\n",
        "# 1) Dictionnaire des meilleurs hyperparam√®tres par entreprise\n",
        "##############################################################################\n",
        "best_params = {\n",
        "    \"Tesla\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 3,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 0.8,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 5,\n",
        "            'min_samples_leaf': 2,\n",
        "            'min_samples_split': 5,\n",
        "            'n_estimators': 50,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 9,\n",
        "            'p': 1,\n",
        "            'weights': 'uniform'\n",
        "        }\n",
        "    },\n",
        "    \"Samsung\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 1,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 5,\n",
        "            'min_samples_leaf': 1,\n",
        "            'min_samples_split': 5,\n",
        "            'n_estimators': 50,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 3,\n",
        "            'p': 2,\n",
        "            'weights': 'distance'\n",
        "        }\n",
        "    },\n",
        "    \"Tencent\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 0.8,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 1,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 10,\n",
        "            'min_samples_leaf': 2,\n",
        "            'min_samples_split': 2,\n",
        "            'n_estimators': 50,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 9,\n",
        "            'p': 2,\n",
        "            'weights': 'uniform'\n",
        "        }\n",
        "    },\n",
        "    \"Alibaba\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 7,\n",
        "            'min_child_weight': 3,\n",
        "            'n_estimators': 200,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': None,\n",
        "            'min_samples_leaf': 1,\n",
        "            'min_samples_split': 2,\n",
        "            'n_estimators': 200,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 3,\n",
        "            'p': 2,\n",
        "            'weights': 'distance'\n",
        "        }\n",
        "    },\n",
        "    \"Sony\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 3,\n",
        "            'n_estimators': 50,\n",
        "            'subsample': 0.8,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 3,\n",
        "            'min_samples_leaf': 1,\n",
        "            'min_samples_split': 2,\n",
        "            'n_estimators': 50,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 9,\n",
        "            'p': 2,\n",
        "            'weights': 'uniform'\n",
        "        }\n",
        "    },\n",
        "    \"Adobe\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 3,\n",
        "            'n_estimators': 50,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 5,\n",
        "            'min_samples_leaf': 2,\n",
        "            'min_samples_split': 5,\n",
        "            'n_estimators': 50,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 9,\n",
        "            'p': 2,\n",
        "            'weights': 'distance'\n",
        "        }\n",
        "    },\n",
        "    \"Johnson & Johnson\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 0.8,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 3,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 5,\n",
        "            'min_samples_leaf': 2,\n",
        "            'min_samples_split': 2,\n",
        "            'n_estimators': 100,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 9,\n",
        "            'p': 2,\n",
        "            'weights': 'uniform'\n",
        "        }\n",
        "    },\n",
        "    \"Pfizer\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 1,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': None,\n",
        "            'min_samples_leaf': 2,\n",
        "            'min_samples_split': 5,\n",
        "            'n_estimators': 100,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 7,\n",
        "            'p': 2,\n",
        "            'weights': 'distance'\n",
        "        }\n",
        "    },\n",
        "    \"Louis Vuitton (LVMH)\": {\n",
        "        \"XGBoost\": {\n",
        "            'colsample_bytree': 1.0,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 3,\n",
        "            'min_child_weight': 3,\n",
        "            'n_estimators': 100,\n",
        "            'subsample': 1.0,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'seed': 42\n",
        "        },\n",
        "        \"RandomForest\": {\n",
        "            'max_depth': 5,\n",
        "            'min_samples_leaf': 2,\n",
        "            'min_samples_split': 5,\n",
        "            'n_estimators': 100,\n",
        "            'random_state': 42\n",
        "        },\n",
        "        \"KNN\": {\n",
        "            'n_neighbors': 9,\n",
        "            'p': 2,\n",
        "            'weights': 'uniform'\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "##############################################################################\n",
        "# 2) Fonctions utilitaires : Chargement, pr√©paration des features\n",
        "##############################################################################\n",
        "def load_close_data(file_path):\n",
        "    \"\"\"Charge un CSV contenant au moins la colonne 'Close'.\n",
        "       Trie par date si la colonne 'Date' existe.\n",
        "       Retourne un DataFrame avec index=Date (si dispo) et colonne 'Close'.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    if 'Date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df = df.sort_values('Date').set_index('Date')\n",
        "\n",
        "    if 'Close' not in df.columns:\n",
        "        raise ValueError(f\"Le fichier {file_path} ne contient pas la colonne 'Close'.\")\n",
        "\n",
        "    return df[['Close']]\n",
        "\n",
        "def create_target_features(df, n=30):\n",
        "    \"\"\"Re√ßoit un array df de shape (m,1).\n",
        "       Retourne X, Y avec une fen√™tre glissante de n=30.\n",
        "    \"\"\"\n",
        "    x, y = [], []\n",
        "    for i in range(n, df.shape[0]):\n",
        "        x.append(df[i-n:i, 0])\n",
        "        y.append(df[i, 0])\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "def prepare_data_for_regression(file_path, n_days=30, test_ratio=0.2):\n",
        "    \"\"\"1) Charge le CSV, r√©cup√®re 'Close'\n",
        "       2) MinMaxScaler\n",
        "       3) Cr√©e X, Y avec create_target_features\n",
        "       4) Split train/test (split temporel)\n",
        "       5) Retourne X_train, Y_train, X_test, Y_test, scaler, close_vals\n",
        "    \"\"\"\n",
        "    df_close = load_close_data(file_path)\n",
        "    close_vals = df_close.values  # shape (m,1)\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    close_scaled = scaler.fit_transform(close_vals)\n",
        "\n",
        "    X, Y = create_target_features(close_scaled, n=n_days)\n",
        "\n",
        "    split_index = int((1 - test_ratio) * len(X))\n",
        "    X_train, Y_train = X[:split_index], Y[:split_index]\n",
        "    X_test,  Y_test  = X[split_index:], Y[split_index:]\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test, scaler, close_vals\n",
        "\n",
        "##############################################################################\n",
        "# 3) Fonctions de r√©gression (avec best params) + r√©gression lin√©aire simple\n",
        "##############################################################################\n",
        "def build_xgb(params):\n",
        "    \"\"\"Construit un XGBRegressor avec les param√®tres donn√©s.\"\"\"\n",
        "    return XGBRegressor(**params)\n",
        "\n",
        "def build_rf(params):\n",
        "    \"\"\"Construit un RandomForestRegressor avec les param√®tres donn√©s.\"\"\"\n",
        "    return RandomForestRegressor(**params)\n",
        "\n",
        "def build_knn(params):\n",
        "    \"\"\"Construit un KNeighborsRegressor avec les param√®tres donn√©s.\"\"\"\n",
        "    return KNeighborsRegressor(**params)\n",
        "\n",
        "def build_linear():\n",
        "    \"\"\"R√©gression lin√©aire, sans hyperparam√®tres.\"\"\"\n",
        "    return LinearRegression()\n",
        "\n",
        "def train_and_predict(model, X_train, Y_train, X_test, Y_test):\n",
        "    \"\"\"Entra√Æne le mod√®le et renvoie (pred, mse, rmse) sur X_test, Y_test.\"\"\"\n",
        "    model.fit(X_train, Y_train)\n",
        "    pred = model.predict(X_test)\n",
        "\n",
        "    mse  = mean_squared_error(Y_test, pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return pred, mse, rmse\n",
        "\n",
        "##############################################################################\n",
        "# 4) Pipeline : pour une entreprise, on charge, on entra√Æne, on affiche\n",
        "##############################################################################\n",
        "def run_for_company(company_name, file_path, n_days=30):\n",
        "    \"\"\"Charge les donn√©es, entra√Æne XGB, RF, KNN (avec best params),\n",
        "       plus LinearRegression, calcule MSE/RMSE, et affiche la courbe.\n",
        "    \"\"\"\n",
        "    # R√©cup√©ration des hyperparam√®tres depuis le dictionnaire\n",
        "    params_xgb = best_params[company_name][\"XGBoost\"]\n",
        "    params_rf  = best_params[company_name][\"RandomForest\"]\n",
        "    params_knn = best_params[company_name][\"KNN\"]\n",
        "\n",
        "    X_train, Y_train, X_test, Y_test, scaler, close_vals = \\\n",
        "        prepare_data_for_regression(file_path, n_days=n_days, test_ratio=0.2)\n",
        "\n",
        "    # Construction des 4 mod√®les\n",
        "    model_xgb = build_xgb(params_xgb)\n",
        "    model_rf  = build_rf(params_rf)\n",
        "    model_knn = build_knn(params_knn)\n",
        "    model_lr  = build_linear()\n",
        "\n",
        "    # Entra√Ænement et pr√©dictions (sur l'√©chelle normalis√©e)\n",
        "    preds_scaled = {}\n",
        "\n",
        "    preds_scaled[\"XGBoost\"], mse_xgb, rmse_xgb = train_and_predict(model_xgb, X_train, Y_train, X_test, Y_test)\n",
        "    preds_scaled[\"RandomForest\"], mse_rf, rmse_rf = train_and_predict(model_rf, X_train, Y_train, X_test, Y_test)\n",
        "    preds_scaled[\"KNN\"], mse_knn, rmse_knn = train_and_predict(model_knn, X_train, Y_train, X_test, Y_test)\n",
        "    preds_scaled[\"Linear\"], mse_lr, rmse_lr = train_and_predict(model_lr, X_train, Y_train, X_test, Y_test)\n",
        "\n",
        "    # Inversion du scaling\n",
        "    Y_test_true = scaler.inverse_transform(Y_test.reshape(-1,1)).ravel()\n",
        "\n",
        "    results = []\n",
        "    preds_true = {}\n",
        "\n",
        "    for model_name, y_scaled in preds_scaled.items():\n",
        "        y_true = scaler.inverse_transform(y_scaled.reshape(-1,1)).ravel()\n",
        "        preds_true[model_name] = y_true\n",
        "\n",
        "        # MSE/RMSE sur l'√©chelle invers√©e\n",
        "        mse_inv = mean_squared_error(Y_test_true, y_true)\n",
        "        rmse_inv = np.sqrt(mse_inv)\n",
        "\n",
        "        # R√©cup√©ration du MSE/RMSE normalis√© (pour l'instant on les recalcule)\n",
        "        mse_norm = mean_squared_error(Y_test, preds_scaled[model_name])\n",
        "        rmse_norm = np.sqrt(mse_norm)\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": model_name,\n",
        "            \"MSE_norm\": mse_norm,\n",
        "            \"RMSE_norm\": rmse_norm,\n",
        "            \"MSE_inversed\": mse_inv,\n",
        "            \"RMSE_inversed\": rmse_inv\n",
        "        })\n",
        "\n",
        "    df_res = pd.DataFrame(results)\n",
        "\n",
        "    # Plot\n",
        "    real_values = close_vals.ravel()\n",
        "    test_start_index = len(close_vals) - len(Y_test_true)\n",
        "    x_axis_pred = range(test_start_index, test_start_index + len(Y_test_true))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(real_values, label=\"Vraies valeurs (Close)\", color='black')\n",
        "    for model_name, y_pred in preds_true.items():\n",
        "        plt.plot(x_axis_pred, y_pred, label=f\"{model_name} Pred\")\n",
        "    plt.title(f\"Pr√©dictions pour {company_name}\")\n",
        "    plt.xlabel(\"Index temporel\")\n",
        "    plt.ylabel(\"Prix de cl√¥ture\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n=== R√©sultats pour {company_name} ===\")\n",
        "    print(df_res)\n",
        "    return df_res\n",
        "\n",
        "##############################################################################\n",
        "# 5) Programme principal : on boucle sur la liste des entreprises\n",
        "##############################################################################\n",
        "##############################################################################\n",
        "# 5) Programme principal : on boucle sur la liste des entreprises\n",
        "##############################################################################\n",
        "def main():\n",
        "    companies = {\n",
        "        \"Tesla\": \"TSLA\",\n",
        "        \"Samsung\": \"005930.KS\",\n",
        "        \"Tencent\": \"TCEHY\",\n",
        "        \"Alibaba\": \"BABA\",\n",
        "        \"Sony\": \"SONY\",\n",
        "        \"Adobe\": \"ADBE\",\n",
        "        \"Johnson & Johnson\": \"JNJ\",\n",
        "        \"Pfizer\": \"PFE\",\n",
        "        \"Louis Vuitton (LVMH)\": \"MC.PA\",\n",
        "    }\n",
        "\n",
        "    data_folder = \"Companies historical data\"  # Dossier o√π se trouvent les CSV\n",
        "    desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "    project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "    data_folder = os.path.join(project_path, \"data\", \"companies\")\n",
        "\n",
        "    all_results = [] # Initialize the list here\n",
        "\n",
        "    for company_name, symbol in companies.items():\n",
        "        # Nom de fichier (ex: \"Tesla_historical_data.csv\")\n",
        "\n",
        "        csv_file = f\"{company_name.replace(' ', '_')}.csv\"\n",
        "        file_path = os.path.join(data_folder, csv_file)\n",
        "\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Fichier introuvable : {file_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n--- {company_name} ({symbol}) ---\")\n",
        "        df_res = run_for_company(company_name, file_path, n_days=30)\n",
        "\n",
        "        # Ajout de la colonne \"Company\" pour identification\n",
        "        df_res[\"Company\"] = company_name\n",
        "        all_results.append(df_res)\n",
        "\n",
        "    # Concat√©nation finale\n",
        "    if len(all_results) > 0:\n",
        "        final_df = pd.concat(all_results, ignore_index=True)\n",
        "        print(\"\\n=== R√©sum√© final ===\")\n",
        "        print(final_df)\n",
        "\n",
        "        # Optionnel : sauvegarder en CSV\n",
        "        # final_df.to_csv(\"final_results_companies.csv\", index=False)\n",
        "    else:\n",
        "        print(\"Aucun fichier trait√©.\")\n",
        "\n",
        "main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22440e2",
      "metadata": {
        "id": "f22440e2"
      },
      "source": [
        "# TP5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad79bdde",
      "metadata": {
        "id": "ad79bdde"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"TP5_Neural_Networks_Corrected.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1Api-Kt2nHTzn3hwhq7QmpI4-jNbzgRCx\n",
        "\n",
        "# TP5 ‚Äì R√©seaux de Neurones pour la Pr√©diction de Valeurs Boursi√®res\n",
        "\"\"\"\n",
        "\n",
        "companies = {\n",
        "            \"Louis Vuitton (LVMH)\": \"MC.PA\",\n",
        "            \"Tesla\": \"TSLA\",\n",
        "        \"Samsung\": \"005930.KS\",\n",
        "        \"Tencent\": \"TCEHY\",\n",
        "        \"Alibaba\": \"BABA\",\n",
        "        \"Sony\": \"SONY\",\n",
        "        \"Adobe\": \"ADBE\",\n",
        "        \"Johnson & Johnson\": \"JNJ\",\n",
        "        \"Pfizer\": \"PFE\",\n",
        "}\n",
        "tickers = list(companies.values())\n",
        "len(tickers)\n",
        "\n",
        "# üì• T√©l√©chargement des donn√©es AAPL\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "df = yf.download(\"AAPL\", start=\"2020-01-01\", end=\"2023-12-31\")[[\"Close\"]]\n",
        "df = df.reset_index()\n",
        "\n",
        "df.columns.name = None\n",
        "df.index.name = None\n",
        "df.head()\n",
        "\n",
        "# üìä Pr√©paration des donn√©es\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def prepare_data(df, feature_col=\"Close\", window_size=30):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(df[[feature_col]])\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(scaled_data)):\n",
        "        X.append(scaled_data[i-window_size:i])\n",
        "        y.append(scaled_data[i])\n",
        "    return np.array(X), np.array(y), scaler\n",
        "\n",
        "X, y, scaler = prepare_data(df)\n",
        "X.shape, y.shape\n",
        "\n",
        "# üîÄ S√©paration des donn√©es\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "X_train.shape, X_test.shape\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# ‚úÖ MLP optimis√©\n",
        "def build_mlp_model(input_shape, hidden_dims=[128, 64, 32], activation='relu', dropout_rate=0.4, optimizer='adam'):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten(input_shape=input_shape))\n",
        "    for dim in hidden_dims:\n",
        "        model.add(tf.keras.layers.Dense(dim, activation=activation, kernel_initializer='glorot_uniform'))\n",
        "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# ‚úÖ RNN optimis√©\n",
        "def build_rnn_model(input_shape, units=100, dropout_rate=0.4, optimizer='adam'):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.SimpleRNN(units, activation='tanh', input_shape=input_shape, return_sequences=False,\n",
        "                                        kernel_initializer='glorot_uniform'))\n",
        "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# ‚úÖ LSTM optimis√©\n",
        "def build_lstm_model(input_shape, units=100, dropout_rate=0.4, optimizer='adam'):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.LSTM(units, activation='tanh', input_shape=input_shape, return_sequences=False,\n",
        "                                   kernel_initializer='glorot_uniform'))\n",
        "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# üöÄ Entra√Ænement g√©n√©rique\n",
        "def train_model(model_type, X_train, y_train, input_shape, **kwargs):\n",
        "\n",
        "    if model_type == \"MLP\":\n",
        "        model = build_mlp_model(input_shape, **kwargs)\n",
        "    elif model_type == \"RNN\":\n",
        "        model = build_rnn_model(input_shape, **kwargs)\n",
        "    elif model_type == \"LSTM\":\n",
        "        model = build_lstm_model(input_shape, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(\"Type de mod√®le inconnu.\")\n",
        "    model.fit(X_train, y_train, epochs=kwargs.get(\"epochs\", 100), batch_size=kwargs.get(\"batch_size\", 32), verbose=kwargs.get(\"verbose\", 0) )\n",
        "    return model\n",
        "\n",
        "def predict(model, X_test, y_test, scaler, model_type):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_rescaled = scaler.inverse_transform(y_pred)\n",
        "    y_test_rescaled = scaler.inverse_transform(y_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\n",
        "\n",
        "    print(f\"{model_type} - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
        "    print(\"\\n10 premi√®res valeurs pr√©dites vs vraies :\")\n",
        "    for p, t in zip(y_pred_rescaled[:10], y_test_rescaled[:10]):\n",
        "        print(f\"Pr√©dit : {p[0]:.2f} | R√©el : {t[0]:.2f}\")\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(y_test_rescaled, label=\"R√©el\")\n",
        "    plt.plot(y_pred_rescaled, label=\"Pr√©dit\")\n",
        "    plt.title(f\"{model_type} - Pr√©dictions vs R√©el\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return mae, rmse\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "print(project_path)\n",
        "\n",
        "for company in companies.keys():\n",
        "    print(f'Training For Company {company}')\n",
        "    tf_model_path = os.path.join(project_path, f\"{company}_models_results\")\n",
        "    os.makedirs(tf_model_path, exist_ok=True)\n",
        "    # 1. Chargement des donn√©es\n",
        "    df = yf.download(companies[company], start=\"2020-01-01\", end=\"2023-12-31\")[[\"Close\"]]\n",
        "    df = df.reset_index()\n",
        "\n",
        "    # 2. Pr√©paration\n",
        "    X, y, scaler = prepare_data(df, window_size=30)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # 3. Tests des mod√®les\n",
        "    results = []\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "\n",
        "    # MLP\n",
        "    model_mlp = train_model(\"MLP\", X_train, y_train, X_train.shape[1:] )\n",
        "    mae_mlp, rmse_mlp = predict(model_mlp, X_test, y_test, scaler, \"MLP\")\n",
        "    results.append([\"MLP\", mae_mlp, rmse_mlp])\n",
        "    model_mlp.save(os.path.join(tf_model_path, \"mlp_model.keras\"))\n",
        "\n",
        "    # RNN\n",
        "    model_rnn = train_model(\"RNN\", X_train, y_train, X_train.shape[1:] )\n",
        "    mae_rnn, rmse_rnn = predict(model_rnn, X_test, y_test, scaler, \"RNN\")\n",
        "    results.append([\"RNN\", mae_rnn, rmse_rnn])\n",
        "    model_rnn.save(os.path.join(tf_model_path, \"rnn_model.keras\"))\n",
        "\n",
        "    # LSTM\n",
        "    model_lstm = train_model(\"LSTM\", X_train, y_train, X_train.shape[1:] )\n",
        "    mae_lstm, rmse_lstm = predict(model_lstm, X_test, y_test, scaler, \"LSTM\")\n",
        "    results.append([\"LSTM\", mae_lstm, rmse_lstm])\n",
        "    model_lstm.save(os.path.join(tf_model_path, \"lstm_model.keras\"))\n",
        "\n",
        "\n",
        "\n",
        "    # R√©sum√©\n",
        "    results_df = pd.DataFrame(results, columns=[\"Mod√®le\", \"MAE\", \"RMSE\"])\n",
        "    results_df.to_csv(os.path.join(tf_model_path, f\"{company}_results.csv\"), index=False)\n",
        "    print(f\"\\nüìä R√©sum√© des performances pour {company}:\")\n",
        "    print(results_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a2ea60f",
      "metadata": {
        "id": "3a2ea60f"
      },
      "source": [
        "# TP6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"TP6_pratique_DS.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1ZJ6A7C41e2YVifCRWNO4bOjFPG3Ww0ig\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def load_existing_news(company_name):\n",
        "    \"\"\"Charge les news existantes depuis un fichier JSON\"\"\"\n",
        "    filename = f\"{company_name.lower().replace(' ', '_')}_news.json\"\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            return json.load(file)\n",
        "    return {}\n",
        "\n",
        "def save_news(company_name, news_dict):\n",
        "    \"\"\"Sauvegarde les news dans un fichier JSON\"\"\"\n",
        "    filename = f\"{company_name.lower().replace(' ', '_')}_news.json\"\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        json.dump(news_dict, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "def get_news_by_date(company_name, api_key):\n",
        "    \"\"\"Scrape les news r√©centes et sauvegarde les nouvelles\"\"\"\n",
        "    url = 'https://newsapi.org/v2/everything'\n",
        "    last_day = datetime.today().strftime('%Y-%m-%d')\n",
        "    first_day = (datetime.today() - timedelta(days=10)).strftime('%Y-%m-%d')\n",
        "\n",
        "    params = {\n",
        "        \"q\": company_name,\n",
        "        \"apiKey\": api_key,\n",
        "        \"language\": \"en\",\n",
        "        \"pageSize\": 100,\n",
        "        \"from\": first_day,\n",
        "        \"to\": last_day,\n",
        "        \"sources\": 'financial-post,the-wall-street-journal,bloomberg,the-washington-post,australian-financial-review'\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    news_dict = load_existing_news(company_name)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        articles = response.json().get('articles', [])\n",
        "        for article in articles:\n",
        "            title = article.get('title', '')\n",
        "            description = article.get('description', '')\n",
        "            published_at = article.get('publishedAt', '')\n",
        "            source_name = article.get('source', {}).get('name', '')\n",
        "\n",
        "            if company_name.lower() in title.lower() or company_name.lower() in description.lower():\n",
        "                date = published_at.split(\"T\")[0]\n",
        "                article_data = {\n",
        "                    'title': title,\n",
        "                    'description': description,\n",
        "                    'source': source_name,\n",
        "                    'date': date\n",
        "                }\n",
        "\n",
        "                # V√©rifie si l'article existe d√©j√†\n",
        "                if date not in news_dict:\n",
        "                    news_dict[date] = []\n",
        "\n",
        "                if not any(existing['title'] == title for existing in news_dict[date]):\n",
        "                    news_dict[date].append(article_data)\n",
        "\n",
        "        # Sauvegarde des nouvelles donn√©es\n",
        "        save_news(company_name, news_dict)\n",
        "        print(f\"‚úîÔ∏è Articles mis √† jour pour {company_name}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Erreur lors de la requ√™te : {response.status_code} - {response.text}\")\n",
        "\n",
        "api_key = \"5fb037a89b2b401cb25a416eca7e5ade\"\n",
        "list_company_name = [\"Tesla\", \"Apple\"]\n",
        "\n",
        "# Corrected definition of all_companies list\n",
        "all_companies = [\n",
        "\"Apple\",\n",
        "\"Microsoft\",\n",
        "\"Amazon\",\n",
        "\"Alphabet\",\n",
        "\"Meta\",\n",
        "\"Tesla\",\n",
        "\"NVIDIA\",\n",
        "\"Samsung\",\n",
        "\"Tencent\",\n",
        "\"Alibaba\",\n",
        "\"IBM\",\n",
        "\"Intel\",\n",
        "\"Oracle\",\n",
        "\"Sony\",\n",
        "\"Adobe\",\n",
        "\"Netflix\",\n",
        "\"AMD\",\n",
        "\"Qualcomm\",\n",
        "\"Cisco\",\n",
        "\"JP Morgan\",\n",
        "\"Goldman Sachs\",\n",
        "\"Visa\",\n",
        "\"Johnson & Johnson\",\n",
        "\"Pfizer\",\n",
        "\"ExxonMobil\",\n",
        "\"ASML\",\n",
        "\"SAP\",\n",
        "\"Siemens\",\n",
        "\"Louis Vuitton (LVMH)\",\n",
        "\"TotalEnergies\",\n",
        "\"Shell\",\n",
        "\"Baidu\",\n",
        "\"JD.com\",\n",
        "\"BYD\",\n",
        "\"ICBC\",\n",
        "\"Toyota\",\n",
        "\"SoftBank\",\n",
        "\"Nintendo\",\n",
        "\"Hyundai\",\n",
        "\"Reliance Industries\",\n",
        "\"Tata Consultancy Services\"\n",
        "]\n",
        "\n",
        "for company_name in list_company_name :\n",
        "  get_news_by_date(company_name, api_key)\n",
        "\n",
        "for company_name in all_companies:\n",
        "  get_news_by_date(company_name, api_key)"
      ],
      "metadata": {
        "id": "XxoHBVHiADAA"
      },
      "id": "XxoHBVHiADAA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "acd74b53",
      "metadata": {
        "id": "acd74b53"
      },
      "source": [
        "# TP 7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade datasets"
      ],
      "metadata": {
        "id": "93wCVjrQTYNN"
      },
      "id": "93wCVjrQTYNN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea86cc7f",
      "metadata": {
        "id": "ea86cc7f"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, concatenate_datasets, DatasetDict, ClassLabel\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
        "from zipfile import ZipFile\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# It's recommended to run this in a separate Colab cell if you need to upgrade datasets\n",
        "# !pip install --upgrade datasets\n",
        "\n",
        "def load_financial_dataset(test_size=0.2, seed=42):\n",
        "    ds1 = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
        "    # ds1 expected schema: \"text\", \"label\" (already ClassLabel or int)\n",
        "\n",
        "    ds2 = load_dataset(\"nickmuchi/financial-classification\")\n",
        "    # ds2 schema from Hub: \"sentence\", \"labels\" (ClassLabel or int)\n",
        "    # Error implies for user: \"text\", \"labels\" or \"text\", \"label\"\n",
        "\n",
        "    train_ds1 = ds1[\"train\"]\n",
        "    train_ds2 = ds2[\"train\"]\n",
        "\n",
        "    # Standardize column names for ds2 to be \"text\" and \"label\"\n",
        "\n",
        "    # 1. Handle label column in ds2\n",
        "    # Ensure it's named \"label\"\n",
        "    if \"labels\" in train_ds2.column_names:\n",
        "        if \"labels\" != \"label\": # Only rename if the name is actually \"labels\"\n",
        "            train_ds2 = train_ds2.rename_column(\"labels\", \"label\")\n",
        "    elif \"label\" not in train_ds2.column_names:\n",
        "        raise ValueError(f\"Label column ('labels' or 'label') not found in ds2 (nickmuchi/financial-classification). Columns: {train_ds2.column_names}\")\n",
        "    # Now ds2 has a \"label\" column.\n",
        "\n",
        "    # 2. Handle text column in ds2\n",
        "    # Ensure it's named \"text\"\n",
        "    if \"sentence\" in train_ds2.column_names:\n",
        "        if \"sentence\" != \"text\": # Only rename if the name is actually \"sentence\"\n",
        "            train_ds2 = train_ds2.rename_column(\"sentence\", \"text\")\n",
        "    elif \"text\" not in train_ds2.column_names:\n",
        "        # This case means neither \"sentence\" nor \"text\" was found.\n",
        "        raise ValueError(f\"Text column ('sentence' or 'text') not found in ds2 (nickmuchi/financial-classification). Columns: {train_ds2.column_names}\")\n",
        "\n",
        "    common_class_label = ClassLabel(num_classes=3, names=['neutral', 'positive', 'negative'])\n",
        "\n",
        "\n",
        "    if train_ds1.features[\"label\"].dtype != common_class_label.dtype or \\\n",
        "       str(train_ds1.features[\"label\"]) != str(common_class_label): # More robust check\n",
        "        train_ds1 = train_ds1.cast_column(\"label\", common_class_label)\n",
        "\n",
        "    if train_ds2.features[\"label\"].dtype != common_class_label.dtype or \\\n",
        "       str(train_ds2.features[\"label\"]) != str(common_class_label):\n",
        "        train_ds2 = train_ds2.cast_column(\"label\", common_class_label)\n",
        "\n",
        "    train_ds1 = train_ds1.filter(lambda example: example['label'] >= 0 and example['label'] < common_class_label.num_classes)\n",
        "    train_ds2 = train_ds2.filter(lambda example: example['label'] >= 0 and example['label'] < common_class_label.num_classes)\n",
        "\n",
        "    full_train = concatenate_datasets([train_ds1, train_ds2])\n",
        "\n",
        "    # Stratified split\n",
        "    split = full_train.train_test_split(test_size=test_size, seed=seed, stratify_by_column=\"label\")\n",
        "\n",
        "    return DatasetDict({\n",
        "        \"train\": split[\"train\"],\n",
        "        \"test\": split[\"test\"]\n",
        "    })\n",
        "\n",
        "\n",
        "dataset = load_financial_dataset()\n",
        "print(dataset)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "MAX_TOKEN_LEN = 512 # Max sequence length for BERT-like models\n",
        "\n",
        "def train_model(model_name, dataset_dict, batch_size, num_epochs):\n",
        "\n",
        "    #desktop = \".\"\n",
        "    desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "    project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "    os.makedirs(project_path, exist_ok=True)\n",
        "\n",
        "    model_save_path = os.path.join(project_path, \"ProsusAI_finbert_results\")\n",
        "    os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "    output_dir = os.path.join(project_path, f\"{model_name.replace('/', '_')}_results\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    # Ensure num_labels matches the dataset (3 for neutral, positive, negative)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_TOKEN_LEN)\n",
        "\n",
        "    # Remove original text column after tokenization to save memory/disk\n",
        "    tokenized_train = dataset_dict[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "    tokenized_test = dataset_dict[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "    # Data collator will dynamically pad the batched examples\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_epochs,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        save_total_limit=2,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        gradient_checkpointing=True,\n",
        "        report_to=\"none\",\n",
        "        disable_tqdm=False,\n",
        "        logging_first_step=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train,\n",
        "        eval_dataset=tokenized_test,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    train_result = trainer.train()\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    print(\"Evaluating model...\")\n",
        "    eval_result = trainer.evaluate()\n",
        "    print(\"√âvaluation finale :\", eval_result)\n",
        "\n",
        "    print(f\"Saving model to {model_save_path}...\")\n",
        "    trainer.save_model(model_save_path)\n",
        "    print(f\"Mod√®le sauvegard√© dans {model_save_path}\")\n",
        "\n",
        "    del model\n",
        "    del trainer\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return eval_result, train_result\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "eval_metrics, train_history = train_model(\"ProsusAI/finbert\", dataset, batch_size=16, num_epochs=3)\n",
        "print(f\"Training time : {time.time() - start_time:.2f} seconds\")\n",
        "print(\"Final Evaluation Metrics:\", eval_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "88Rxkz96U91C"
      },
      "id": "88Rxkz96U91C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "295e1ec6",
      "metadata": {
        "id": "295e1ec6"
      },
      "source": [
        "# TP 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf3e824a",
      "metadata": {
        "id": "bf3e824a"
      },
      "outputs": [],
      "source": [
        "\n",
        "#! pip install transformers\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import pytz\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from matplotlib.lines import Line2D\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "model_name = \"ProsusAI_finbert\"\n",
        "desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "model_save_path = os.path.join(project_path, \"ProsusAI_finbert_results\")\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "output_dir = os.path.join(project_path, f\"{model_name.replace('/', '_')}_results\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def get_text_timestamps(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        news_data = json.load(file)\n",
        "\n",
        "    texts = []\n",
        "    timestamps = []\n",
        "\n",
        "    for date, articles in news_data.items():\n",
        "        for item in articles:\n",
        "            try:\n",
        "                utc_time = datetime.strptime(item['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "                ny_time = utc_time.replace(tzinfo=pytz.utc).astimezone(pytz.timezone('America/New_York'))\n",
        "                rounded_time = ny_time.replace(minute=0, second=0, microsecond=0)\n",
        "\n",
        "                title = item.get('title', '')\n",
        "                description = item.get('description', '')\n",
        "                full_text = f\"{title} {description}\".strip()\n",
        "\n",
        "                if full_text:\n",
        "                    texts.append(full_text)\n",
        "                    timestamps.append(rounded_time)\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur sur un article : {e}\")\n",
        "                continue\n",
        "\n",
        "    return texts, timestamps\n",
        "\n",
        "# put all json files in a folder called json files with the json files inside\n",
        "# The folder should be in the same directory as main file\n",
        "# -- json_files -- Nintendo_news.json\n",
        "#   |            |  ....\n",
        "#   |           -- ,,,news.json\n",
        "# -- main.py\n",
        "get_text_timestamps('json_files/Nintendo_news.json')\n",
        "\n",
        "with open('json_files/Nintendo_news.json', 'r', encoding='utf-8') as file:\n",
        "    news_data = json.load(file)\n",
        "\n",
        "def get_sentiment(model_path, texts):\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "    sentiments = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            sentiment = torch.argmax(logits, dim=1).item()\n",
        "            sentiments.append(sentiment)\n",
        "\n",
        "    return sentiments\n",
        "\n",
        "def align_timestamps(timestamps):\n",
        "    aligned = []\n",
        "    for ts in timestamps:\n",
        "        ts = pd.Timestamp(ts).tz_localize(None)  # Pour uniformiser avec Pandas\n",
        "        market_open = ts.replace(hour=9, minute=30, second=0, microsecond=0)\n",
        "        market_close = ts.replace(hour=15, minute=0, second=0, microsecond=0)\n",
        "\n",
        "        if market_open <= ts < market_close:\n",
        "            aligned.append(ts)\n",
        "        elif market_close <= ts < ts.replace(hour=23, minute=59, second=59):\n",
        "            aligned.append(market_close)\n",
        "        else:\n",
        "            # Entre minuit et 9h30 => mapper √† 15h la veille\n",
        "            prev_day = (ts - pd.Timedelta(days=1)).replace(hour=15, minute=0, second=0, microsecond=0)\n",
        "            aligned.append(prev_day)\n",
        "    return aligned\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "ts = datetime(2025, 3, 15, 8, 45, tzinfo=pytz.timezone(\"America/New_York\"))\n",
        "aligned_ts = align_timestamps([ts])[0]\n",
        "print(aligned_ts)\n",
        "# ‚ûú Doit afficher : 2025-03-14 15:00:00\n",
        "def plot_comparison(df, sentiment_a, sentiment_b, timestamps, title_a, title_b):\n",
        "    aligned_ts = align_timestamps(timestamps)\n",
        "\n",
        "    # Rendre les timestamps tz-naive pour √©viter l'erreur de comparaison\n",
        "    df['Datetime'] = df['Datetime'].dt.tz_localize(None)\n",
        "    aligned_ts = [ts.replace(tzinfo=None) for ts in aligned_ts]\n",
        "\n",
        "    # Grouper les sentiments par timestamp\n",
        "    def group_sentiments(ts_list, sentiments):\n",
        "        grouped = defaultdict(list)\n",
        "        for ts, sent in zip(ts_list, sentiments):\n",
        "            grouped[ts].append(sent)\n",
        "        return grouped\n",
        "\n",
        "    grouped_a = group_sentiments(aligned_ts, sentiment_a)\n",
        "    grouped_b = group_sentiments(aligned_ts, sentiment_b)\n",
        "\n",
        "    # Pour chaque timestamp, calculer le nombre de news par sentiment\n",
        "    def get_points(grouped):\n",
        "        points = []\n",
        "        for ts, sents in grouped.items():\n",
        "            for i, sent in enumerate(sents):\n",
        "                points.append((ts, sent, i))\n",
        "        return points\n",
        "\n",
        "    points_a = get_points(grouped_a)\n",
        "    points_b = get_points(grouped_b)\n",
        "\n",
        "    sentiment_colors = {2: 'green', 1: 'gold', 0: 'red'}\n",
        "    sentiment_labels = {2: 'Positif', 1: 'Neutre', 0: 'N√©gatif'}\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=True)\n",
        "\n",
        "    for ax, points, title in zip(axes, [points_a, points_b], [title_a, title_b]):\n",
        "        # Tracer la courbe de prix\n",
        "        ax.plot(df['Datetime'], df['Close'], color='blue', label='Prix action')\n",
        "        # Superposer les news\n",
        "        for ts, sent, idx in points:\n",
        "            # Trouver le timestamp le plus proche dans df['Datetime']\n",
        "            nearest_ts = df['Datetime'].iloc[(df['Datetime'] - ts).abs().argsort()[0]]\n",
        "            y = df.loc[df['Datetime'] == nearest_ts, 'Close']\n",
        "            if not y.empty:\n",
        "                y_val = y.values[0] + (idx - 0.5) * 0.5  # D√©calage vertical\n",
        "                ax.scatter(nearest_ts, y_val, color=sentiment_colors[sent], s=80, edgecolor='black', zorder=5)\n",
        "        # Ajouter le titre du subplot\n",
        "        ax.set_title(title)\n",
        "\n",
        "    axes[0].set_ylabel('Prix de cl√¥ture')\n",
        "\n",
        "    # L√©gende personnalis√©e\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='blue', lw=2, label='Prix action'),\n",
        "        Line2D([0], [0], marker='o', color='w', label='Positif', markerfacecolor='green', markersize=10, markeredgecolor='black'),\n",
        "        Line2D([0], [0], marker='o', color='w', label='Neutre', markerfacecolor='gold', markersize=10, markeredgecolor='black'),\n",
        "        Line2D([0], [0], marker='o', color='w', label='N√©gatif', markerfacecolor='red', markersize=10, markeredgecolor='black')\n",
        "    ]\n",
        "    axes[1].legend(handles=legend_elements, loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_company_sentiment(ticker, news_path, model_a_path, model_b_path, title_a, title_b):\n",
        "    print(f\"\\nTraitement de {ticker}...\")\n",
        "\n",
        "    # 1. R√©cup√©rer les news\n",
        "    texts, timestamps = get_text_timestamps(news_path)\n",
        "\n",
        "    # 2. Pr√©dire les sentiments avec les deux mod√®les\n",
        "    sentiments_a = get_sentiment(model_a_path, texts)\n",
        "    sentiments_b = get_sentiment(model_b_path, texts)\n",
        "\n",
        "    # 3. R√©cup√©rer les donn√©es de march√©\n",
        "    df_stock = get_stock_data(ticker)\n",
        "\n",
        "    # 4. Afficher les graphiques\n",
        "    plot_comparison(df_stock, sentiments_a, sentiments_b, timestamps, title_a, title_b)\n",
        "\n",
        "def get_stock_data(ticker, start_date=\"2025-01-01\"):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    df = stock.history(start=start_date, interval=\"60m\")\n",
        "    df = df.reset_index()\n",
        "    df.rename(columns={\"Datetime\": \"Datetime\", \"Close\": \"Close\"}, inplace=True)\n",
        "    return df[[\"Datetime\", \"Close\"]]\n",
        "\n",
        "companies = {\n",
        "        \"Amazon\": \"AMZN\",\n",
        "\n",
        "            \"Louis Vuitton (LVMH)\": \"MC.PA\",\n",
        "            \"Tesla\": \"TSLA\",\n",
        "        \"Samsung\": \"005930.KS\",\n",
        "        \"Tencent\": \"TCEHY\",\n",
        "        \"Alibaba\": \"BABA\",\n",
        "        \"Sony\": \"SONY\",\n",
        "        \"Adobe\": \"ADBE\",\n",
        "        \"Johnson & Johnson\": \"JNJ\",\n",
        "        \"Pfizer\": \"PFE\",\n",
        "}\n",
        "\n",
        "for company_name , ticker in companies.items():\n",
        "    adjusted_company_name = company_name.replace(\" \", \"_\")\n",
        "    news_path = f\"json_files/{adjusted_company_name}_news.json\"\n",
        "    analyze_company_sentiment(\n",
        "        ticker=ticker,\n",
        "        news_path=news_path,\n",
        "        model_a_path=\"ProsusAI/finbert\",              # FinBERT original\n",
        "        model_b_path=model_save_path,      # Ton mod√®le fine-tun√©\n",
        "        title_a=\"FinBERT (original)\",\n",
        "        title_b=\"FinBERT (fine-tun√©)\"\n",
        "    )\n",
        "    print(f\"Traitement de {company_name} termin√©.\")\n",
        "    print(\"-\" * 50)\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import pytz\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from matplotlib.lines import Line2D\n",
        "import numpy as np\n",
        "\n",
        "# Configuration\n",
        "model_name = \"ProsusAI_finbert\"\n",
        "desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
        "project_path = os.path.join(desktop, \"Projet_Final_DS\")\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "\n",
        "# Paths for models and outputs\n",
        "model_save_path = os.path.join(project_path, \"ProsusAI_finbert_results\")\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# New directory for TP8 plots\n",
        "tp8_output_dir = os.path.join(project_path, \"tp8_plots\")\n",
        "os.makedirs(tp8_output_dir, exist_ok=True)\n",
        "\n",
        "def get_text_timestamps(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        news_data = json.load(file)\n",
        "\n",
        "    texts = []\n",
        "    timestamps = []\n",
        "\n",
        "    for date, articles in news_data.items():\n",
        "        for item in articles:\n",
        "            try:\n",
        "                utc_time = datetime.strptime(item['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "                ny_time = utc_time.replace(tzinfo=pytz.utc).astimezone(pytz.timezone('America/New_York'))\n",
        "                rounded_time = ny_time.replace(minute=0, second=0, microsecond=0)\n",
        "\n",
        "                title = item.get('title', '')\n",
        "                description = item.get('description', '')\n",
        "                full_text = f\"{title} {description}\".strip()\n",
        "\n",
        "                if full_text:\n",
        "                    texts.append(full_text)\n",
        "                    timestamps.append(rounded_time)\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur sur un article : {e}\")\n",
        "                continue\n",
        "\n",
        "    return texts, timestamps\n",
        "\n",
        "\n",
        "def get_sentiment(model_path, texts):\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "    model.eval()\n",
        "    sentiments = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            sentiment = torch.argmax(outputs.logits, dim=1).item()\n",
        "            sentiments.append(sentiment)\n",
        "    return sentiments\n",
        "\n",
        "\n",
        "def align_timestamps(timestamps):\n",
        "    aligned = []\n",
        "    for ts in timestamps:\n",
        "        ts = pd.Timestamp(ts).tz_localize(None)\n",
        "        market_open = ts.replace(hour=9, minute=30)\n",
        "        market_close = ts.replace(hour=15, minute=0)\n",
        "        if market_open <= ts < market_close:\n",
        "            aligned.append(ts)\n",
        "        elif market_close <= ts:\n",
        "            aligned.append(market_close)\n",
        "        else:\n",
        "            prev_day = (ts - pd.Timedelta(days=1)).replace(hour=15, minute=0)\n",
        "            aligned.append(prev_day)\n",
        "    return aligned\n",
        "\n",
        "\n",
        "def get_stock_data(ticker, start_date=\"2025-01-01\"):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    df = stock.history(start=start_date, interval=\"60m\").reset_index()\n",
        "    return df[[\"Datetime\", \"Close\"]]\n",
        "\n",
        "\n",
        "def plot_comparison(df, sentiment_a, sentiment_b, timestamps, title_a, title_b, company_name, output_dir):\n",
        "    aligned_ts = align_timestamps(timestamps)\n",
        "    # Ensure tz-naive\n",
        "    df['Datetime'] = df['Datetime'].dt.tz_localize(None)\n",
        "    aligned_naive = [ts.replace(tzinfo=None) for ts in aligned_ts]\n",
        "\n",
        "    def group_sentiments(ts_list, sents):\n",
        "        grouped = defaultdict(list)\n",
        "        for ts, sent in zip(ts_list, sents):\n",
        "            grouped[ts].append(sent)\n",
        "        return grouped\n",
        "\n",
        "    grouped_a = group_sentiments(aligned_naive, sentiment_a)\n",
        "    grouped_b = group_sentiments(aligned_naive, sentiment_b)\n",
        "\n",
        "    def get_points(grouped):\n",
        "        pts = []\n",
        "        for ts, s_list in grouped.items():\n",
        "            for idx, sent in enumerate(s_list):\n",
        "                pts.append((ts, sent, idx))\n",
        "        return pts\n",
        "\n",
        "    points_a = get_points(grouped_a)\n",
        "    points_b = get_points(grouped_b)\n",
        "\n",
        "    sentiment_colors = {2:'green',1:'gold',0:'red'}\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18,6), sharey=True)\n",
        "    for ax, pts, title in zip(axes, [points_a, points_b],[title_a,title_b]):\n",
        "        ax.plot(df['Datetime'], df['Close'], label='Prix action')\n",
        "        for ts, sent, idx in pts:\n",
        "            nearest = df['Datetime'].iloc[(df['Datetime']-ts).abs().argsort()[0]]\n",
        "            y = df.loc[df['Datetime']==nearest,'Close'].values\n",
        "            if y.size:\n",
        "                y_val = y[0] + (idx-0.5)*0.5\n",
        "                ax.scatter(nearest, y_val, color=sentiment_colors[sent], s=80, edgecolor='black', zorder=5)\n",
        "        ax.set_title(title)\n",
        "    axes[0].set_ylabel('Prix de cl√¥ture')\n",
        "\n",
        "    legend_elements = [\n",
        "        Line2D([0],[0],color='blue',lw=2,label='Prix action'),\n",
        "        Line2D([0],[0],marker='o',color='w',label='Positif',markerfacecolor='green',markersize=10,markeredgecolor='black'),\n",
        "        Line2D([0],[0],marker='o',color='w',label='Neutre',markerfacecolor='gold',markersize=10,markeredgecolor='black'),\n",
        "        Line2D([0],[0],marker='o',color='w',label='N√©gatif',markerfacecolor='red',markersize=10,markeredgecolor='black')\n",
        "    ]\n",
        "    axes[1].legend(handles=legend_elements, loc='upper left')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot\n",
        "    save_path = os.path.join(output_dir, f\"{company_name.replace(' ','_')}_tp8.png\")\n",
        "    fig.savefig(save_path)\n",
        "    print(f\"üì∏ Plot saved to {save_path}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# Main loop for TP8\n",
        "companies = {\n",
        "    \"Amazon\": \"AMZN\",\n",
        "    \"Louis Vuitton (LVMH)\": \"MC.PA\",\n",
        "    \"Tesla\": \"TSLA\",\n",
        "    \"Samsung\": \"005930.KS\",\n",
        "    \"Tencent\": \"TCEHY\",\n",
        "    \"Alibaba\": \"BABA\",\n",
        "    \"Sony\": \"SONY\",\n",
        "    \"Adobe\": \"ADBE\",\n",
        "    \"Johnson & Johnson\": \"JNJ\",\n",
        "    \"Pfizer\": \"PFE\",\n",
        "}\n",
        "\n",
        "for company_name, ticker in companies.items():\n",
        "    adjusted = company_name.replace(' ', '_')\n",
        "    news_path = os.path.join('json_files', f\"{adjusted}_news.json\")\n",
        "    texts, timestamps = get_text_timestamps(news_path)\n",
        "    sents_a = get_sentiment('ProsusAI/finbert', texts)\n",
        "    sents_b = get_sentiment(model_save_path, texts)\n",
        "    df_stock = get_stock_data(ticker)\n",
        "\n",
        "    plot_comparison(\n",
        "        df_stock, sents_a, sents_b, timestamps,\n",
        "        \"FinBERT (original)\", \"FinBERT (fine-tun√©)\",\n",
        "        company_name, tp8_output_dir\n",
        "    )\n",
        "    print(f\"Traitement de {company_name} termin√©.\")\n"
      ],
      "metadata": {
        "id": "gjdC74Ojog3W"
      },
      "id": "gjdC74Ojog3W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls /root/Desktop/Projet_Final_DS/tp8_plots/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0mJmgMloh0U",
        "outputId": "592a9b77-95bc-4d0e-939c-d167d9595449"
      },
      "id": "q0mJmgMloh0U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Adobe_tp8.png\t\t     'Louis_Vuitton_(LVMH)_tp8.png'   Tencent_tp8.png\n",
            " Alibaba_tp8.png\t      Pfizer_tp8.png\t\t      Tesla_tp8.png\n",
            " Amazon_tp8.png\t\t      Samsung_tp8.png\n",
            "'Johnson_&_Johnson_tp8.png'   Sony_tp8.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSC0MjOxpIKE"
      },
      "id": "jSC0MjOxpIKE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}